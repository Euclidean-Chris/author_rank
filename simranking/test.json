[
{"title": ["security, privacy "], "Author": "JonCrowcroft", "Abstract": ["This article is an editorial note submitted to CCR. It has NOT been peer reviewed. The author takes full responsibility for this article\u2019s technical content. Comments can be posted through CCR Online. In all seriousness, Differential Privacy is a new technique and set of tools for managing responses to statistical queries over secured data, in such a way that the user cannot reconstruct more precise identification of principles in the dataset beyond a formally well-specified bound. This means that personally sensitive data such as Internet packet traces or social network measurements can be shared between researchers without invading personal privacy, and that assurances can be made with accuracy. With less seriousness, I would like to talk about Differential Piracy, but not without purpose. For sure, while there are legitimate reasons for upstanding citizens to live without fear of eternal surveillance, there is also a segment of society that gets away with things they shouldn\u2019t, under a cloak. Perhaps that is the (modest) price we have to pay for a modicum less paranoia in this brave new world. So, there has been a lot of work recently on Piracy Preserving Queries and Differential Piracy. These two related technologies exploit new ideas in statistical security. Rather than security through obscurity, the idea is to offer privacy through lack of differentiation (no, not inability to perform basic calculus, more the inability to distinguish between large numbers of very similar things)."], "doi": "10.1.1.258.5022", "Keywords": ["differential piracy", "internet packet trace", "legitimate reason", "brave new world", "large number", "statistical query", "social network measurement", "user cannot reconstruct", "piracy preserving query", "statistical security", "new technique", "editorial note", "ccr online", "sensitive data", "new idea", "differential privacy", "eternal surveillance", "basic calculus", "full responsibility", "precise identification", "personal privacy", "related technology", "similar thing", "well-specified bound", "technical content"], "Labels": "security privacy"},
{"title": ["Artificial Evolution for Computer Graphics  (1991) "], "Author": "KarlSims", "Abstract": ["This paper describes how evolutionary techniques of variation and selection can be used to create complex simulated structures, textures, and motions for use in computer graphics and animation. Interactive selection, based on visual perception of procedurally generated results, allows the user to direct simulated evolutions in preferred directions. Several examples using these methods have been implemented and are described. 3D plant structures are grown using fixed sets of genetic parameters. Images, solid textures, and animations are created using mutating symbolic lisp expressions. Genotjps consisting of symbolic expressions are presented as an attempt to surpass the limitations of fixed-length genotypes with predefine expression rules. his proposed that artificial evolution has potential as a powerful tool for achieving flexible complexity with a minimum of user input and knowledge of details. 2"], "doi": "10.1.1.161.2547", "Keywords": ["computer graphic", "artificial evolution", "symbolic expression", "simulated evolution", "plant structure", "flexible complexity", "interactive selection", "evolutionary technique", "fixed-length genotype", "fixed set", "complex simulated structure", "solid texture", "predefine expression rule", "genetic parameter", "powerful tool", "several example", "visual perception", "preferred direction", "symbolic lisp expression", "user input"], "Labels": "computer graphics"},
{"title": ["Computer Vision  (1982) "], "Author": "KusumaKumariB.M", "Abstract": ["Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye\u2019s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break."], "doi": "10.1.1.675.9058", "Keywords": ["computer vision", "last stage", "motor vehicle accident", "main cause", "main aim", "second stage", "different approach", "efficient performance", "behavioral element", "driver inattention", "traffic accident", "facial feature", "low quality webcam", "crucial parameter", "first stage", "eye status", "fatigued driver", "main stage", "infrared light", "complex problem", "human eye"], "Labels": "computer vision"},
{"title": ["Semantic Security: Privacy Definitions Revisited "], "Author": "JinfeiLiu,LiXiong,JunLuo", "Abstract": ["Abstract. In this paper \u2217 we illustrate a privacy framework named Indistinguishable \u2020 Privacy. In-distinguishable privacy could be deemed as the formalization of the existing privacy definitions in privacy preserving data publishing as well as secure multi-party computation. We introduce three representative privacy notions in the literature, Bayes-optimal privacy for privacy preserving data publishing, differential privacy for statistical data release, and privacy w.r.t. semi-honest behavior in the secure multi-party computation setting, and prove they are equivalent. To the best of our knowl-edge, this is the first work that illustrates the relationships of these privacy definitions and unifies them through one framework."], "doi": "10.1.1.699.9996", "Keywords": [], "Labels": "security privacy"},
{"title": [" Data Security  (1979) "], "Author": "DorothyE.Denning,PeterJ.Denning", "Abstract": ["The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area."], "doi": "10.1.1.81.5750", "Keywords": ["data security", "confidential information", "statistical query", "general audience", "data bank", "little background", "general nature", "personal privacy", "active user", "whmh data set", "statlstmal data bank", "data set", "protect statistical database", "inherent limitation", "much interest", "unauthorized disclosure", "data encryption attempt", "techmcal safeguard"], "Labels": "security privacy"},
{"title": ["Privacy Preserving Data Mining  (2000) "], "Author": "YehudaLindell,BennyPinkas", "Abstract": ["In this paper we address the issue of privacy preserving data mining. Specifically, we consider a  scenario in which two parties owning confidential databases wish to run a data mining algorithm on  the union of their databases, without revealing any unnecessary information. Our work is motivated  by the need to both protect privileged information and enable its use for research or other purposes. The"], "doi": "10.1.1.10.5402", "Keywords": ["privacy preserving data mining", "confidential database", "data mining", "data mining algorithm", "unnecessary information"], "Labels": "security privacy"},
{"title": ["Ubiquitous Security: Privacy versus Protection "], "Author": "TimothyK.Buennemeyer,RandolphC.Marchany,JosephG.Tront", "Abstract": ["Abstract- In the ambient computing future, security promises to be the foundational design feature that allows pervasive systems to protect personal information privacy. As fledgling pervasive computing systems gain a foothold presence in becoming more flexible and, at the same time, more invisibly interconnected, system users may have to trade privacy and protection to gain full entry into this new information-laden environment. This paper examines the current state-of-the-shelf security components, which predominantly overlay wired and wireless networks that will support next generation pervasive systems with a defense-in-depth approach. Information technology \u201cbest practices \u201d are considered, and privacy concerns within typical networks are discussed. An examination of emerging privacy protecting technologies is presented, and Radio Frequency Identification (RFID) tags and legal concerns are discussed to portray the asymmetry of information flow in pervasive systems that can impact our personal information privacy."], "doi": "10.1.1.94.817", "Keywords": ["ubiquitous security", "privacy versus protection", "pervasive system", "personal information privacy", "security promise", "foundational design feature", "next generation pervasive system", "legal concern", "new information-laden environment", "defense-in-depth approach", "radio frequency identification", "foothold presence", "information flow", "privacy concern", "current state-of-the-shelf security component", "full entry", "system user", "wireless network", "information technology", "typical network"], "Labels": "security privacy"},
{"title": ["Secure spread spectrum watermarking for multimedia  (1997) "], "Author": "IngemarJ.Cox,JoeKilian,F.ThomsonLeighton,TalalShamoon", "Abstract": [" This paper presents a secure (tamper-resistant) algorithm for watermarking images, and a methodology for digital watermarking that may be generalized to audio, video, and multimedia data. We advocate that a watermark should be constructed as an independent and identically distributed (i.i.d.) Gaussian random vector that is imperceptibly inserted in a spread-spectrum-like fashion into the perceptually most significant spectral components of the data. We argue that insertion of a watermark under this regime makes the watermark robust to signal processing operations (such as lossy compression, filtering, digital-analog and analog-digital conversion, requantization, etc.), and common geometric transformations (such as cropping, scaling, translation, and rotation) provided that the original image is available and that it can be succesfully registered against the transformed watermarked image. In these cases, the watermark detector unambiguously identifies the owner. Further, the use of Gaussian noise, ensures strong resilience to multiple-document, or collusional, attacks. Experimental results are provided to support these claims, along with an exposition of pending open problems. "], "doi": "10.1.1.118.9444", "Keywords": ["secure spread spectrum", "lossy compression", "watermark detector", "multimedia data", "signal processing operation", "open problem", "significant spectral component", "gaussian noise", "gaussian random vector", "transformed watermarked image", "watermark robust", "strong resilience", "digital watermarking", "original image", "spread-spectrum-like fashion", "experimental result", "common geometric transformation", "analog-digital conversion"], "Labels": "multimedia"},
{"title": ["Interior-Point Algorithm: Theory and Analysis  (1996) "], "Author": "YinyuYe", "Abstract": [" "], "doi": "10.1.1.111.4071", "Keywords": ["interior-point algorithm"], "Labels": "algorithm theory"},
{"title": ["Maximum likelihood from incomplete data via the EM algorithm  (1977) "], "Author": "A.P.Dempster,N.M.Laird,D.B.Rubin", "Abstract": ["A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.\r\n"], "doi": "10.1.1.133.4884", "Keywords": ["incomplete data", "em algorithm", "maximum likelihood", "applicable algorithm", "monotone behaviour", "various level", "factor analysis", "maximum likelihood estimate", "value situation", "many example", "variance component estimation", "finite mixture model"], "Labels": "algorithm theory"},
{"title": ["Algorithmic Game Theory  (2009) "], "Author": "TimRoughgarden", "Abstract": [], "doi": "10.1.1.178.2859", "Keywords": ["algorithmic game theory"], "Labels": "algorithm theory"},
{"title": ["Differential privacy  and robust statistics   (2009) "], "Author": "CynthiaDwork,JingLei", "Abstract": ["We show by means of several examples that robust statistical estimators present an excellent starting point for differentially private estimators. Our algorithms use a new paradigm for differentially private mechanisms, which we call Propose-Test-Release (PTR), and for which we give a formal definition and general composition theorems."], "doi": "10.1.1.167.4515", "Keywords": ["general term", "new paradigm", "statistical estimator", "private mechanism", "excellent starting point", "several example", "private estimator", "formal definition", "general composition theorem"], "Labels": "security privacy"},
{"title": ["Security, Privacy and Anonymity  (2004) "], "Author": "ThomasWright", "Abstract": ["A good share of the Internet's popularity is due to the widespread image of it being totally anonymous. The truth, however, is somewhat different. Anonimity and privacy are no matter of course, but human rights, and their infiltration was never closer than today. The following paper will shed light on the modern means of private and commercial espionage and discuss precautions we can take to protect our privacy in the information age."], "doi": "10.1.1.58.9363", "Keywords": ["following paper", "discus precaution", "information age", "widespread image", "modern mean", "good share", "commercial espionage", "human right"], "Labels": "security privacy"},
{"title": ["The Nature of Statistical Learning Theory  (1999) "], "Author": "VladimirN.Vapnik", "Abstract": [" Statistical learning theory was introduced in the late 1960\u2019s. Until the 1990\u2019s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990\u2019s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more"], "doi": "10.1.1.332.356", "Keywords": ["statistical learning theory", "theoretical analysis", "support vector machine", "new type", "new algorithmic approach", "general overview", "function estimation", "classical statistical paradigm", "abstract statistical", "practical algorithm", "developed theory", "abstract learning theory", "algorithmic aspect", "multidimensional function", "estimation problem"], "Labels": "algorithm theory"},
{"title": [" Security, privacy, and confidentiality issues  "], "Author": "GrantKelly,BruceMckenzie", "Abstract": [], "doi": "10.1.1.97.8213", "Keywords": ["confidentiality issue"], "Labels": "security privacy"},
{"title": ["Shadow algorithms for computer graphics  (1977) "], "Author": "FranklinC.Crow", "Abstract": ["Shadows are advocated for improved comprehension and enhanced realism in computer-synthesized images. A classification of shadow algorithms delineates three approaches: shadow computation during scanout; division of object surfaces into shadowed and unshadowed areas prior to removal of hidden surfaces; and inclusion of shadow volumes in the object data. The classes are related to existing shadow algorithms and implementations within each class are sketched. A brief comparison of the three approaches suggests that the last approach has the most appealing characteristics."], "doi": "10.1.1.424.6834", "Keywords": ["shadow algorithm", "computer graphic", "appealing characteristic", "object surface", "shadow computation", "object data", "computer-synthesized image", "unshadowed area", "brief comparison", "shadow volume", "last approach", "improved comprehension", "hidden surface"], "Labels": "computer graphics"},
{"title": ["Computer Graphics  (2009) "], "Author": "JohnE.Howland", "Abstract": ["An introduction to some of the key ideas in computer graphics is given. Modeling, 2D and 3D viewing, transformations and related ideas from linear algebra are presented. Subject Areas: Computer Graphics."], "doi": "10.1.1.180.3813", "Keywords": ["computer graphic", "linear algebra", "key idea", "related idea", "subject area"], "Labels": "computer graphics"},
{"title": ["The Amoeba Distributed Operating System  (1992) "], "Author": "AndrewS.Tanenbaum,GregoryJ.Sharp,DeBoelelaanA", "Abstract": ["INTRODUCTION  Roughly speaking, we can divide the history of modern computing into the following eras:  d 1970s: Timesharing (1 computer with many users)  d 1980s: Personal computing (1 computer per user)  d 1990s: Parallel computing (many computers per user) Until about 1980, computers were huge, expensive, and located in computer centers. Most organizations had a single large machine. In the 1980s, prices came down to the point where each user could have his or her own personal computer or workstation. These machines were often networked together, so that users could do remote logins on other people's computers or share files in various (often ad hoc) ways. Nowadays some systems have many processors per user, either in the form of a parallel computer or a large collection of CPUs shared by a small user community. Such systems are usually called parallel or distributed computer systems.  This devel"], "doi": "10.1.1.37.8404", "Keywords": ["amoeba distributed operating system", "introduction roughly speaking", "single large machine", "large collection", "small user community", "computer system", "personal computer", "computer center", "many user", "parallel computer", "many processor", "share file", "ad hoc", "personal computing", "following era", "many computer"], "Labels": "operating systems"},
{"title": ["Interval Analysis For Computer Graphics  (1992) "], "Author": "JohnM.Snyder", "Abstract": ["This paper discusses how interval analysis can be used to solve a wide variety of problems in computer graphics. These problems include ray tracing, interference detection, polygonal decomposition of parametric surfaces, and CSG on solids bounded by parametric surfaces. Only two basic algorithms are required: SOLVE, which computes solutions to a system of constraints, and MINIMIZE, which computes the global minimum of a function, subject to a system of constraints. We present algorithms for SOLVE and MINIMIZE using interval analysis as the conceptual framework. Crucial to the technique is the creation of \"inclusion functions\" for each constraint and function to be minimized. Inclusion functions compute a bound on the range of a function, given a similar bound on its domain, allowing a branch and bound approach to constraint solution and constrained minimization. Inclusion functions also allow the MINIMIZE  algorithm to compute global rather than local minima, unlike many other numerica..."], "doi": "10.1.1.56.7431", "Keywords": ["interval analysis", "computer graphic", "inclusion function", "parametric surface", "bound approach", "global minimum", "present algorithm", "ray tracing", "basic algorithm", "local minimum", "similar bound", "interference detection", "wide variety", "polygonal decomposition", "conceptual framework", "many numerica", "minimize algorithm"], "Labels": "computer graphics"},
{"title": ["Computer Graphics  (1996) "], "Author": "DavidDobkin,SethTeller", "Abstract": ["INTRODUCTION  Computer graphics is often given as a prime application area for the techniques of computational geometry. The histories of the two fields have a great deal of overlap, with similar methods (e.g. sweep-line and area subdivision algorithms) arising independently in each. Both fields have often focused on similar problems, although with different computational models. For example, hidden surface removal (visible surface identification) is a fundamental problem of computer graphics. This problem has also motivated many researchers in computational geometry. At the same time, as the fields have matured, they have brought different requirements to similar problems. Here, we aim to highlight both similarities and differences between the fields. Computational geometry is fundamentally concerned with the efficient quantitative representation and manipulation of ideal geometric entities to produce exact results. Computer graphics shares these goals, in part. However, graphi"], "doi": "10.1.1.53.2169", "Keywords": ["computer graphic", "computational geometry", "similar problem", "computer graphic share", "ideal geometric entity", "visible surface identification", "similar method", "introduction computer graphic", "efficient quantitative representation", "prime application area", "great deal", "different computational model", "area subdivision algorithm", "many researcher", "fundamental problem", "hidden surface removal", "exact result", "different requirement"], "Labels": "computer graphics"},
{"title": ["VLFeat -- An open and portable library of computer vision algorithms  (2010) "], "Author": "AndreaVedaldi,etal.", "Abstract": [" "], "doi": "10.1.1.176.1934", "Keywords": ["portable library", "computer vision algorithm"], "Labels": "computer vision"},
{"title": ["Pervasive Computing: Vision and Challenges  (2001) "], "Author": "M.Satyanarayanan", "Abstract": ["This paper discusses the challenges in computer systems research posed by the emerging field of pervasive computing. It first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. It then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. Next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. The paper closes with a discussion of the research necessary to develop these capabilities."], "doi": "10.1.1.24.6338", "Keywords": ["pervasive computing", "effective use", "key capability", "mobile computing", "uneven conditioning", "computer system research", "new research thrust", "new field", "smart space"], "Labels": "computer vision"},
{"title": ["Markov Random Field Models in Computer Vision  (1994) "], "Author": "S.Z.Li", "Abstract": [". A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles. 1 Introduction  Since its beginning in early 1960's, computer vision research has been evolving from heuristic design of algorithms to syste..."], "doi": "10.1.1.33.9687", "Keywords": ["markov random field model", "computer vision", "prior model", "likelihood model", "unified approach", "computer vision problem", "sound mathematical principle", "probability estimate", "heuristic design", "true labeling", "problem domain dependent", "various prior constraint", "high level computer vision", "high level object recognition", "latter relates", "recent advance", "former depends", "posterior probability", "contextual constraint", "bayesian labeling", "systematic approach", "prior probability", "computer vision research"], "Labels": "computer vision"},
{"title": ["Scale-Space Theory in Computer Vision  (1994) "], "Author": "TonyLindeberg", "Abstract": ["A basic problem when deriving information from measured data, such as images, originates from the fact that objects in the world, and hence image structures, exist as meaningful entities only over certain ranges of scale. \"Scale-Space Theory in Computer Vision\" describes a formal theory for representing the notion of scale in image data, and shows how this theory applies to essential problems in computer vision such as computation of image features and cues to surface shape. The subjects range from the mathematical foundation to practical computational techniques. The power of the methodology is illustrated by a rich set of examples.\r\n\r\nThis book is the first monograph on scale-space theory. It is intended as an introduction, reference, and inspiration for researchers, students, and system designers in computer vision as well as related fields such as image processing, photogrammetry, medical image analysis, and signal processing in general.\r\n\r\nFor more information, please see http://www.nada.kth.se/~tony/book.html "], "doi": "10.1.1.11.3577", "Keywords": ["computer vision", "scale-space theory", "essential problem", "system designer", "medical image analysis", "certain range", "basic problem", "practical computational technique", "formal theory", "image feature", "image data", "mathematical foundation", "tony book", "rich set", "meaningful entity", "signal processing", "first monograph", "image structure", "image processing", "related field"], "Labels": "computer vision"},
{"title": ["Computer support for knowledge-building communities  (1994) "], "Author": "MarleneScardamalia,CarlBereiter", "Abstract": ["Nobody wants to use technology to recreate education as it is, yet there is not much to distinguish what goes on in most computer-supported classrooms versus traditional classrooms. Kay (1991) has suggested that the phenomenon of reframing innovations to recreate the familiar is itself commonplace. Thus, one sees all manner of powerful technology (Hypercard, CD-ROM, Lego Logo, and so forth) used to conduct shopworn school activities: copying material from one resource into another (e.g., using Hypercard to assemble sound and visual bites produced by others), and following step-by-step procedures (e.g., creating Lego Logo machines by following steps in a manual). With new technologies, student-generated collages and reproductions appear more inventive and sophisticated-with impressive displays of sound, video, and typography-but from a cognitive perspective, it is not clear what, if any, knowledge content has been processed by the students. In this chapter we offer a suggestion for how to escape the pattern of reinventing the familiar with educational technology. Knowledge-building discourse is at the heart of the superior education that we have in mind. We argue that the classroom needs to foster"], "doi": "10.1.1.600.463", "Keywords": ["computer support", "knowledge-building community", "shopworn school activity", "sophisticated-with impressive display", "knowledge content", "student-generated collage", "cognitive perspective", "powerful technology", "traditional classroom", "lego logo", "superior education", "visual bite", "lego logo machine", "knowledge-building discourse", "new technology", "educational technology", "computer-supported classroom", "step-by-step procedure"], "Labels": "computer education"},
{"title": ["Human-Computer Interaction  (1993) "], "Author": "AlanDix,SandraCairncross,GilbertCockton,RussellBeale,RobertStAmant,MarthaHause", "Abstract": ["www.bcs-hci.org.uk Find out what happened at HCI2004 Interacting with \u2026 music aeroplanes petrol pumps Published by the British HCI Group \u2022 ISSN 1351-119X 1"], "doi": "10.1.1.117.1491", "Keywords": ["human-computer interaction", "music aeroplane", "uk find", "british hci group issn"], "Labels": "human computer interaction"},
{"title": ["Internet of Things Security Privacy Trust "], "Author": "UnknownAuthors", "Abstract": ["zzini 5"], "doi": "10.1.1.728.3487", "Keywords": [], "Labels": "security privacy"},
{"title": ["SOCIAL NETWORKING: SECURITY, PRIVACY, AND APPLICATIONS BY "], "Author": "SoniaJahid", "Abstract": ["Online social networks have become ubiquitous and changed the way that users interact online. There has been an enormous growth in the usage of online social networking in the past few years as users share a variety of information including personal profiles, pictures, and messages to socialize with their friends in the Internet. Besides, several special purpose social networks have emerged to serve their users with useful functionalities. This vast amount of user data is valuable, and therefore, introduce several security and privacy risks and challenges. In this thesis we propose several techniques to enhance the security and privacy features of online social networks. Our goal is to shift the control over user data from a centralized social network provider to the end users. We realize this concept by decentralization of the social networking architecture. First, we construct and implement a cryptographic access control mecha-nism that ensures data confidentiality and integrity, and efficiently supports"], "doi": "10.1.1.855.1531", "Keywords": [], "Labels": "security privacy"},
{"title": ["Pfam protein families database "], "Author": "RobertD.Finn,JohnTate,JainaMistry,PennyC.Coggill,StephenJohnSammut,Hans-rudolfHotz,GoranCeric,KristofferForslund,SeanR.Eddy,ErikL.L.Sonnhammer,AlexBateman", "Abstract": ["Pfam is a comprehensive collection of protein domains and families, represented as multiple sequence alignments and as profile hidden Markov models. The current release of Pfam (22.0) contains 9318 protein families. Pfam is now based not only on the UniProtKB sequence database, but also on NCBI GenPept and on sequences from selected metage-nomics projects. Pfam is available on the web from the consortium members using a new, consistent and improved website design in the UK"], "doi": "10.1.1.1008.9771", "Keywords": [], "Labels": "database"},
{"title": [" Some philosophical problems from the standpoint of artificial intelligence  (1969) "], "Author": "JohnMcCarthy,PatrickJ.Hayes", "Abstract": [" "], "doi": "10.1.1.381.877", "Keywords": ["artificial intelligence", "philosophical problem"], "Labels": "artificial intelligence"},
{"title": ["MediaBench: A Tool for Evaluating and Synthesizing Multimedia and Communications Systems  "], "Author": "ChunhoLee,MiodragPotkonjak,WilliamH.Mangione-smith", "Abstract": ["Over the last decade, significant advances have been made in compilation technology for capitalizing on instruction-level parallelism (ILP). The vast majority of ILP compilation research has been conducted in the context of generalpurpose computing, and more specifically the SPEC benchmark suite. At the same time, a number of microprocessor architectures have emerged which have VLIW and SIMD structures that are well matched to the needs of the ILP compilers. Most of these processors are targeted at embedded applications such as multimedia and communications, rather than general-purpose systems. Conventional wisdom, and a history of hand optimization of inner-loops, suggests that ILP compilation techniques are well suited to these applications. Unfortunately, there currently exists a gap between the compiler community and embedded applications developers. This paper presents MediaBench, a benchmark suite that has been designed to fill this gap. This suite has been constructed through a three-step process: intuition and market driven initial selection, experimental measurement to establish uniqueness, and integration with system synthesis algorithms to establish usefulness."], "doi": "10.1.1.12.2091", "Keywords": ["communication system", "synthesizing multimedia", "experimental measurement", "instruction-level parallelism", "last decade", "three-step process", "significant advance", "compiler community", "compilation technology", "spec benchmark suite", "application developer", "general-purpose system", "embedded application", "vast majority", "benchmark suite", "hand optimization", "ilp compiler", "conventional wisdom", "initial selection", "microprocessor architecture", "ilp compilation research", "generalpurpose computing", "simd structure", "ilp compilation technique", "system synthesis algorithm"], "Labels": "multimedia"},
{"title": ["FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets  (1995) "], "Author": "ChristosFaloutsos,King-Ip(David)Lin", "Abstract": ["A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example' type (which translates to a range query); the `all pairs' query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret..."], "doi": "10.1.1.39.5767", "Keywords": ["fast algorithm", "multimedia datasets", "domain expert", "fine-tuned spatial access method", "fast searching", "efficient ret", "distance information", "feature extraction function", "k-dimensional space", "k-d space", "range query", "promising idea", "several type", "similarity distance", "feature-extraction function", "spatial join", "best-match query", "multimedia database"], "Labels": "multimedia"},
{"title": ["A Scheme for Real-Time Channel Establishment in Wide-Area Networks  (1990) "], "Author": "DomenicoFerrari,DineshC.Verma", "Abstract": ["Multimedia communication involving digital audio and/or digital video has rather strict delay requirements. A real-time channel is defined in this paper as a simplex connection between a source and a destination characterized by parameters representing the performance requirements of the client. A real-time service is capable of creating realtime channels on demand and guaranteeing their performance. These guarantees often take the form of lower bounds on the bandwidth allocated to a channel and upper bounds on the delays to be experienced by a packet on the channel. In this paper"], "doi": "10.1.1.24.6802", "Keywords": ["wide-area network", "real-time channel establishment", "simplex connection", "performance requirement", "real-time channel", "digital audio", "strict delay requirement", "digital video", "multimedia communication", "real-time service", "upper bound", "realtime channel"], "Labels": "multimedia"},
{"title": ["Planning Algorithms  (2004) "], "Author": "StevenMLaValle", "Abstract": ["This book presents a unified treatment of many different kinds of planning algorithms. The subject lies at the crossroads between robotics, control theory, artificial intelligence, algorithms, and computer graphics. The particular subjects covered include motion planning, discrete planning, planning under uncertainty, sensor-based planning, visibility, decision-theoretic planning, game theory, information spaces, reinforcement learning, nonlinear systems, trajectory planning, nonholonomic planning, and kinodynamic planning."], "doi": "10.1.1.1.7086", "Keywords": ["many different kind", "computer graphic", "trajectory planning", "particular subject", "discrete planning", "unified treatment", "reinforcement learning", "control theory", "nonlinear system", "sensor-based planning", "decision-theoretic planning", "game theory", "nonholonomic planning", "include motion planning", "kinodynamic planning", "information space", "artificial intelligence"], "Labels": "algorithm theory"},
{"title": ["The Viterbi algorithm  (1973) "], "Author": "G.DavidForney", "Abstract": ["vol. 6, no. 8, pp. 211-220, 1951. [7] J. L. Anderson and J. W..Ryon, \u201cElectromagnetic radiation in accelerated systems, \u201d Phys. Rev., vol. 181, pp. 1765-1775, 1969. [8] C. V. Heer, \u201cResonant frequencies of an electromagnetic cavity in an accelerated system of reference, \u201d Phys. Reu., vol. 134, pp. A799-A804, 1964. [9] T. C. Mo, \u201cTheory of electrodynamics in media in noninertial frames and applications, \u201d J. Math. Phys., vol. 11, pp. 2589-2610, 1970."], "doi": "10.1.1.412.6372", "Keywords": ["viterbi algorithm", "accelerated system", "electromagnetic cavity", "noninertial frame", "electromagnetic radiation", "resonant frequency"], "Labels": "algorithm theory"},
{"title": ["Randomized Algorithms  (1995) "], "Author": "RajeevMotwani", "Abstract": ["Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from"], "doi": "10.1.1.399.7777", "Keywords": ["randomized algorithm", "many application", "major benefit", "random number", "computational number theory", "running time", "widespread application"], "Labels": "algorithm theory"},
{"title": ["The geometry of algorithms with orthogonality constraints  (1998) "], "Author": "AlanEdelman,Tom\u00e1sA.Arias,StevenT.Smith", "Abstract": ["  In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper. "], "doi": "10.1.1.129.2433", "Keywords": ["orthogonality constraint", "new algorithm", "new insight", "signal processing", "top level mathematical view", "nonlinear eigenvalue problem", "numerical linear algebra algorithm", "unrelated algorithm", "perturbation theory", "compare algorithm", "electronic structure computation", "symmetric eigenvalue problem", "new newton", "stiefel manifold", "gradient algorithm", "geometrical framework"], "Labels": "algorithm theory"},
{"title": ["A Maximum Entropy approach to Natural Language Processing  (1996) "], "Author": "AdamL.Berger,StephenA.DellaPietra,VincentJ.DellaPietra", "Abstract": ["The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  "], "doi": "10.1.1.103.7637", "Keywords": ["natural language processing", "maximum entropy approach", "maximum entropy", "multiple thread", "pattern recognition", "maximum entropy model", "widescale application", "statistical estimation", "biblical time", "statistical modeling", "maximum-likelihood approach", "example several problem", "real world problem"], "Labels": "natural language processing"},
{"title": ["Monitors: An Operating System Structuring Concept  (1974) "], "Author": "C.A.RHoare", "Abstract": ["This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author's original work. This paper develops Brinch-Hansen's concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:"], "doi": "10.1.1.91.3720", "Keywords": ["operating system structuring concept", "single rcsourcc scheduler", "possible rnctltotl", "operating system", "original work", "suitable proof rule", "buffer pool", "system implementation", "key word", "illustrative example", "accurate copy", "bounded buffer", "cr category", "mutual exclusion", "disk head optimizer", "digitized copy", "alarm clock"], "Labels": "operating systems"},
{"title": ["Exokernel: An Operating System Architecture for Application-Level Resource Management  (1995) "], "Author": "DawsonR.Engler,M.FransKaashoek,JamesO\u2019toole", "Abstract": ["We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10\u2013100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS\u2019s application-level virtual memory and IPC primitives are 5\u201350 times faster than Ultrix\u2019s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1"], "doi": "10.1.1.100.6047", "Keywords": ["operating system architecture", "application-level resource management", "machine resource", "traditional operating system abstraction", "unprecedented degree", "application-specific customization", "ipc primitive", "primitive aegis operation", "untrusted application-level operating system", "exos implement process", "system design", "mature monolithic unix", "prototype exokernel-based system", "untrusted software", "virtual memory", "resource revocation", "exokernel architecture", "inter-process communication abstraction", "physical hardware resource", "excellent combination", "application-level virtual memory", "low-level interface", "ultrix primitive", "handle event"], "Labels": "operating systems"},
{"title": ["Computer Graphics "], "Author": "DonP.Mitchell", "Abstract": ["Nonuniform sampling of images is a useful technique in computer graphics, because a properly designed pattern of samples can make aliasing take the form of high-frequency random noise. In this paper, the tech-nique of nonuniform sampling is extended from two dimensions to include the extra parameter dimensions of distribution ray tracing. A condition for optimality is suggested, and algorithms for approximating opti-mal sampling are developed. The technique is demonstrated at low sampling densities, so the characteris-tics of aliasing noise are clearly visible. At supersampling rates, this technique should move noise into fre-quencies above the passband of the pixel-reconstruction filter."], "doi": "10.1.1.556.445", "Keywords": ["computer graphic", "nonuniform sampling", "distribution ray tracing", "high-frequency random noise", "opti-mal sampling", "useful technique", "extra parameter dimension", "pixel-reconstruction filter"], "Labels": "computer graphics"},
{"title": ["Extensibility, safety and performance in the SPIN operating system  (1995) "], "Author": "BrianN.Bershad,StefanSavage,PrzemysPardyak,EminGunSirer,MarcE.Fiuczynski,DavidBecker,CraigChambers,SusanEggers", "Abstract": ["This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system's interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1"], "doi": "10.1.1.117.6702", "Keywords": ["spin operating system", "system service", "operating system", "core set", "extensible operating system", "extension infrastructure", "kernel address space", "link-time mechanism", "er extension rapid access", "dec alpha workstation", "type safe language", "extensible service", "operating system kernel", "allow application", "ne-grained interface", "operating system code", "particular level"], "Labels": "operating systems"},
{"title": ["Computer Graphics and "], "Author": "RicardoLopes,VisualizationGroup,KenHilf,LukeJayapalan,RafaelBidarra", "Abstract": ["The nature of most modern mobile games is different from that of most computer/console games, which are typically targeted at casual gamers and are played in a wide vari-ety of space, time and device contexts. We argue that this feature of mobile games naturally fits with adaptive proce-dural content generation (PCG). In this paper, we propose the integration of two PCG-based approaches (experience-driven and context-driven PCG) to support the generation of adaptive mobile game levels. We present and discuss the implementation of our approach in an existing game, 7\u2019s Wild Ride. Gameplay semantics and player modeling are used to steer a level generator, featuring a time-dependent dynamic difficulty adjustment mechanism. From our two user studies, we conclude that (i) context-driven levels are preferable over traditional ones, and (ii) the game can adapt to different player types, keeping its gameplay balanced and player satisfaction."], "doi": "10.1.1.708.4766", "Keywords": [], "Labels": "computer graphics"},
{"title": ["Contiki - a Lightweight and Flexible Operating System for Tiny Networked Sensors  (2004) "], "Author": "AdamDunkels,Bj\u00f6rnGr\u00f6nvall,ThiemoVoigt", "Abstract": ["of tiny networked devices that communicate untethered. For large scale networks it is important to be able to dynamically download code into the network. In this paper we present Contiki, a lightweight operating system with support for dynamic loading and replacement of individual programs and services. Contiki is built around an event-driven kernel but provides optional preemptive multithreading that can be applied to individual processes. We show that dynamic loading and unloading is feasible in a resource constrained environment, while keeping the base system lightweight and compact."], "doi": "10.1.1.59.2303", "Keywords": ["tiny networked sensor", "flexible operating system", "dynamic loading", "optional preemptive multithreading", "base system", "individual process", "large scale network", "tiny networked device", "event-driven kernel", "individual program", "present contiki"], "Labels": "operating systems"},
{"title": ["TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems  (1994) "], "Author": "PeteKeleher,AlanL.Cox,SandhyaDwarkadas,WillyZwaenepoel", "Abstract": ["TreadMarks is a distributed shared memory (DSM) system for standard Unix systems such as  SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on  Ultrix using DECstation-5000/240's that are connected by a 100-Mbps switch-based ATM LAN  and a 10-Mbps Ethernet. Our objective is to determine the efficiency of a user-level DSM implementation  on commercially available workstations and operating systems.  We achieved good speedups on the 8-processor ATM network for Jacobi (7.4), TSP (7.2), Quicksort  (6.3), and ILINK (5.7). For a slightly modified version of Water from the SPLASH benchmark  suite, we achieved only moderate speedups (4.0) due to the high communication and synchronization  rate. Speedups decline on the 10-Mbps Ethernet (5.5 for Jacobi, 6.5 for TSP, 4.2 for  Quicksort, 5.1 for ILINK, and 2.1 for Water), reflecting the bandwidth limitations of the Ethernet.  These results support the contention that, with suitable networking technology, DSM is a..."], "doi": "10.1.1.27.9177", "Keywords": ["operating system", "shared memory", "standard workstation", "10-mbps ethernet", "synchronization rate", "bandwidth limitation", "user-level dsm implementation", "splash benchmark suite", "distributed shared memory", "good speedup", "high communication", "speedup decline", "8-processor atm network", "standard unix system", "available workstation", "performance evaluation", "moderate speedup", "100-mbps switch-based atm lan"], "Labels": "operating systems"},
{"title": ["A Survey of Computer Vision-Based Human Motion Capture  (2001) "], "Author": "ThomasB.Moeslund,ErikGranum", "Abstract": ["A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition. Each process is discussed and divided into subprocesses and/or categories of methods to provide a reference to describe and compare the more than 130 publications covered by the survey. References are included throughout the paper to exemplify important issues and their relations to the various methods. A number of general assumptions used in this research field are identified and the character of these assumptions indicates that the research field is still in an early stage of development. To evaluate the state of the art, the major application areas are identified and performances are analyzed in light of the methods"], "doi": "10.1.1.108.203", "Keywords": ["computer vision-based human motion capture", "research field", "various method", "general assumption", "general overview", "important issue", "system functionality", "early stage", "comprehensive survey", "pose estimation", "major application area"], "Labels": "computer vision"},
{"title": ["Computer Graphics  (2013) "], "Author": "Jos\u00e9CarlosMiranda,SupervisorProf,Ver\u00f3nicaOrvalho,Co-supervisorProf,AugustoSousa", "Abstract": ["ii This research is partially supported by: Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia (SFRH/BD/46588/2008), Instituto de Telecomunica\u00e7\u00f5es, and the projects:"], "doi": "10.1.1.915.1896", "Keywords": [], "Labels": "computer graphics"},
{"title": ["Computer Graphics "], "Author": "Fl\u00e1vioMello,EdilbertoStrauss,Ant\u00f4nioOliveira,AlineGesualdi", "Abstract": ["The performance of a walkthrough over terrain models is deeply influenced by the real scenario high level of details. To guarantee natural and smooth changes in a sequence of scenes, it is necessary to display the actual height field's maps at interactive frame rates. These frame rates can be accomplished by reducing the number of rendered geometric primitives without compromising the visual quality. This paper describes an optimized algorithm for building a triangular mesh, the terrain model, which combines an efficient regular grid representation with low cost memory requirements, using a bottom-up approach. Terrain mesh simplification, quadtree, digital elevation model. 1."], "doi": "10.1.1.144.1736", "Keywords": ["computer graphic", "terrain model", "bottom-up approach", "terrain mesh simplification", "optimized algorithm", "frame rate", "smooth change", "digital elevation model", "real scenario high level", "rendered geometric primitive", "efficient regular grid representation", "low cost memory requirement", "visual quality", "triangular mesh", "actual height field", "interactive frame rate"], "Labels": "computer graphics"},
{"title": ["Computer vision "], "Author": "KaiYu,AlexeiEfros(cmu,AdaptedFromDavidKriegman", "Abstract": ["What is computer vision? 3"], "doi": "10.1.1.717.1149", "Keywords": [], "Labels": "computer vision"},
{"title": ["Inverse Rendering for Computer Graphics  (1998) "], "Author": "StephenRobertMarschner", "Abstract": ["Creating realistic images has been a major focus in the study of computer graphics for much of its history. This effort has led to mathematical models and algorithms that can compute predictive, or physically realistic, images from known camera positions and scene descriptions that include the geometry of objects, the reflectance of surfaces, and the lighting used to illuminate the scene. These images accurately describe the physical quantities that would be measured from a real scene. Because these algorithms can predict real images, they can also be used in inverse problems to work backward from photographs to attributes of the scene. Work on three such inverse rendering problems is described. The first, inverse lighting, assumes knowledge of geometry, reflectance, and the recorded photograph and solves for the lighting in the scene. A technique using a linear least-squares system is proposed and demonstrated. Also demonstrated is an application of inverse lighting, called re-lighting, which modi es lighting in photographs. The second two inverse rendering problems solve for unknown reflectance, given images with known geometry, lighting, and camera positions. Photographic texture measurement"], "doi": "10.1.1.122.7873", "Keywords": ["computer graphic", "inverse rendering", "inverse lighting", "camera position", "inverse rendering problem", "real image", "known geometry", "unknown reflectance", "scene description", "realistic image", "real scene", "major focus", "e lighting", "mathematical model", "inverse problem", "linear least-squares system", "physical quantity", "photographic texture measurement"], "Labels": "computer graphics"},
{"title": ["A Bayesian computer vision system for modeling human interactions  (2000) "], "Author": "NuriaM.Oliver,BarbaraRosario,AlexP.Pentland", "Abstract": [" We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic \u00aaAlife-style\u00ba training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training.  "], "doi": "10.1.1.132.3753", "Keywords": ["human interaction", "bayesian computer vision system", "limited training data", "bottom-up information", "machine learning system", "flexible prior model", "human behavior", "real human behavior", "priori model", "different state-based learning architecture", "statistical bayesian approach", "visual surveillance task", "interaction behavior", "synthetic alife-style training system", "real-time computer vision", "additional tuning", "closed feedback loop", "chmm model"], "Labels": "computer vision"},
{"title": ["Statistical Models of Appearance for Computer Vision  (2000) "], "Author": "T.F.Cootes,C.J.Taylor", "Abstract": [" "], "doi": "10.1.1.27.5016", "Keywords": ["computer vision", "statistical model"], "Labels": "computer vision"},
{"title": ["Visual interpretation of hand gestures for human-computer interaction: A review  (1997) "], "Author": "VladimirI.Pavlovic,RajeevSharma,ThomasS.Huang", "Abstract": [" The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient \u201cpurposive\u201d approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of human-computer interaction.  "], "doi": "10.1.1.353.4090", "Keywords": ["hand gesture", "human-computer interaction", "visual interpretation", "human hand", "efficient purposive approach", "index term vision-based gesture recognition", "real-time requirement", "computational advance", "natural mode", "elaborate modeling", "gesture recognition", "vision-based gesture recognition", "implemented gestural system", "gesture interpretation approach", "gesture analysis", "attractive alternative", "humancomputer interaction", "nonrigid motion analysis", "active research area", "interface device", "important difference", "computer vision-based analysis", "hand model", "appearance-based model", "current progress", "future research", "hand tracking", "image appearance model", "computational hurdle", "potential application"], "Labels": "human computer interaction"},
{"title": ["Seven principles for good practice in undergraduate education  (1987) "], "Author": "W.Chickering,ZeldaF.Gamson", "Abstract": ["Apathetic students, illiterate graduates, incompetent teaching, impersonal campuses-- so rolls the drumfire of criticism of higher education. More than two years of reports have spelled out the problems. States have been quick to respond by holding out carrots and beating with sticks. There are neither enough carrots nor enough sticks to improve undergraduate education without the commitment and action of students and faculty members. They are the precious resources on whom the improvement of undergraduate education depends. But how can students and faculty members improve undergraduate education? Many campuses around the country are asking this question. To provide a focus for their work, we offer seven principles based on research on good teaching and learning in colleges and universities. Good practice in undergraduate education: 1. Encourages contacts between students and faculty. 2. Develops reciprocity and cooperation among students. 3. Uses active learning techniques. 4. Gives prompt feedback. 5. Emphasizes time on task. 6. Communicates high expectations. 7. Respects diverse talents and ways of learning. We can do it ourselves-- with a little bit of help.... A Focus for Improvement These seven principles are not ten commandments shrunk to a twentieth century attention span. They are intended as guidelines for faculty members, students, and administrators-- with support from state agencies and trustees-- to improve teaching and learning. These principles seem like good common sense, and they are-- because many teachers and students have experienced them and because research supports them. They rest on 50 years of research on the way teachers teach and students learn, how students work and play with one another, and how students and faculty talk to each other. While each practice can stand alone on its own, when all are present their effects multiply. Together they employ six powerful forces in education:"], "doi": "10.1.1.529.3875", "Keywords": ["undergraduate education", "good practice", "faculty member", "seven principle", "ten commandment", "good teaching", "twentieth century attention span", "precious resource", "give prompt feedback", "emphasizes time", "communicates high expectation", "illiterate graduate", "way teacher", "encourages contact", "many campus", "impersonal campus", "develops reciprocity", "good common sense", "respect diverse talent", "little bit", "enough stick", "powerful force", "state agency", "incompetent teaching", "many teacher", "faculty talk", "apathetic student", "enough carrot"], "Labels": "computer education"},
{"title": ["Distributed Cognition: Toward a New Foundation for Human-Computer Interaction Research  (2000) "], "Author": "JamesHollan,EdwinHutchins,DavidKirsh", "Abstract": ["We are quickly passing through the historical moment when people work in front of a single computer, dominated by a small CRT and focused on tasks involving only local information. Networked computers are becoming ubiquitous and are playing increasingly significant roles in our lives and in the basic infrastructures of science, business, and social interaction. For human-computer interaction to advance in the new millennium we need to better understand the emerging dynamic of interaction in which the focus task is no longer confined to the desktop but reaches into a complex networked world of information and computer-mediated interactions. We think the theory of distributed cognition has a special role to play in understanding interactions between people and technologies, for its focus has always been on whole environments: what we really do in them and how we coordinate our activity in them. Distributed cognition provides a radical reorientation of how to think about designing and supporting human-computer interaction. As a theory it is specifically tailored to understanding interactions among people and technologies. In this article we propose distributed cognition as a new foundation for human-computer interaction, sketch an integrated research framework, and use selections from our earlier work to suggest how this framework can provide new opportunities in the design of digital work materials."], "doi": "10.1.1.135.6076", "Keywords": ["new foundation", "human-computer interaction", "human-computer interaction research", "computer-mediated interaction", "special role", "distributed cognition", "understanding interaction", "whole environment", "complex networked world", "local information", "focus task", "historical moment", "basic infrastructure", "new millennium", "research framework", "small crt", "digital work material", "single computer", "significant role", "radical reorientation", "new opportunity", "social interaction"], "Labels": "human computer interaction"},
{"title": ["  THE CAUSAL EFFECT OF EDUCATION ON EARNINGS  (1999) "], "Author": "DavidCard", "Abstract": [], "doi": "10.1.1.175.9130", "Keywords": ["causal effect education earnings"], "Labels": "computer education"},
{"title": ["Formalising trust as a computational concept  (1994) "], "Author": "StephenPaulMarsh", "Abstract": ["Trust is a judgement of unquestionable utility \u2014 as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say \u201cI trust you, \u201d but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area. Summary 1. Overview This thesis presents an overview of trust as a social phenomenon and discusses it formally. It argues that trust is: \u2022 A means for understanding and adapting to the complexity of the environment. \u2022 A means of providing added robustness to independent agents. \u2022 A useful judgement in the light of experience of the behaviour of others. \u2022 Applicable to inanimate others. The thesis argues these points from the point of view of artificial agents. Trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. Moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. This has been done, and is documented in the thesis. 2. Exposition There are places in the thesis where it is necessary to give a broad outline before going deeper. In consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (This is particularly apparent in the first and second chapters.) To present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. Chapter 3 expands, depth first, presenting critical views of established researchers."], "doi": "10.1.1.102.8227", "Keywords": ["artificial agent", "computational concept", "future work", "additional tool", "second chapter", "thorough treatment", "human trust", "informal use", "broad outline", "independent agent", "distributed artificial intelligence", "imperfect understanding", "introductory chapter", "everyday life", "detailed exploration", "critical view", "precise discussion", "added robustness", "thorough understanding", "useful judgement", "proper understanding", "social phenomenon", "trust-based decision", "unquestionable utility"], "Labels": "computer education"},
{"title": ["Intelligent agents: Theory and practice  (1995) "], "Author": "MichaelWooldridge,NicholasR.Jennings", "Abstract": ["The concept of an agent has become important in both Artificial Intelligence (AI) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the prop-erties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology."], "doi": "10.1.1.119.2204", "Keywords": ["intelligent agent", "practical issue", "agent technology", "mainstream computer science", "agent language", "important issue", "agent theory", "mathematical formalism", "agent architecture", "tutorial introduction", "agent theorist", "short review", "software engineering model", "hardware system", "potential application", "artificial intelligence", "software system"], "Labels": "intelligent agents"},
{"title": ["and Human-Computer Interaction "], "Author": "RonanFitzpatrick,CatherineHiggins,RonanFitzpatrick,CatherineHiggins", "Abstract": ["synthesis of software quality: European Community law and human-computer interaction."], "doi": "10.1.1.169.9143", "Keywords": ["human-computer interaction", "software quality", "european community law"], "Labels": "human computer interaction"},
{"title": ["Artificial Intelligence "], "Author": "BerlinChen,Ai\u2013berlinChen", "Abstract": ["\u2022 The theoretical and practical issues for all disciplines of Artificial Intelligence (AI) will be considered \u2013 AI is interdisciplinary!"], "doi": "10.1.1.704.9267", "Keywords": [], "Labels": "artificial intelligence"},
{"title": ["BALTIC CONFERENCE Human \u2013 Computer Interaction "], "Author": "BaltischeSommerschule,Bezugsm\u00f6glichkeitenUniversit\u00e4tRostock,Universit\u00e4tRostock,WissenschaftsverbundIuk,DruckUniversit\u00e4tsdruckereiRostock", "Abstract": ["Human \u2013 Computer Interaction"], "doi": "10.1.1.460.8321", "Keywords": ["baltic conference human computer interaction", "human computer interaction"], "Labels": "human computer interaction"},
{"title": ["Distributed Database Systems "], "Author": "M.Tamer\u00d6zsu", "Abstract": ["this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control."], "doi": "10.1.1.33.2276", "Keywords": ["database system", "architectural design issue", "replication control", "basic dbms function", "distributed dbms technology", "query processing", "concurrency control", "data distribution"], "Labels": "database"},
{"title": ["Artificial Intelligence  (1982) "], "Author": "StuartC.Shapiro", "Abstract": ["This article is a revised version of Shapiro, S. C. \"Artificial Intelligence,\" in S. C. Shapiro, Ed. Encyclopedia of Artificial Intelligence, Second Edition. New York: John Wiley & Sons, 1991.  engaged in by people, is commonly taken as being part of human intelligent cognitive behavior. It is acceptable, though not required, if the implemented model perform some tasks better than any person would. Bearing in mind Church's Thesis (see Church, Alonzo), this goal might be reworded as asking the question, \"Is intelligence a computable function?\" In the AI areas of computer vision (q.v.) and robotics (q.v.), computational philosophy is sometimes replaced by computational natural philosophy (science). For example, some computer vision researchers are interested in the computational optics question of how the information contained in light waves reflected from an object can be used to reconstruct the object. Notice that this is a different question from the computational psychology question of how the human visual system uses light waves falling on the retina to identify objects in the world, or even the computational philosophy question of how any intelligent entity could use light waves falling on a two-dimensional retinal grid to discriminate one three-dimensional object-in-the-world from a set of other possible objects."], "doi": "10.1.1.33.323", "Keywords": ["artificial intelligence", "light wave", "ai area", "possible object", "computational optic question", "computational philosophy question", "human intelligent cognitive behavior", "human visual system", "computational psychology question", "three-dimensional object-in-the-world", "john wiley son", "computer vision", "new york", "implemented model", "revised version", "mind church", "computational philosophy", "second edition", "computable function", "computational natural philosophy", "different question", "computer vision researcher", "two-dimensional retinal grid", "intelligent entity"], "Labels": "artificial intelligence"},
{"title": ["The Pfam protein families database  (2002) "], "Author": "AlexBateman,LachlanCoin,RichardDurbin,RobertD.Finn,VolkerHollich,AjayKhanna,MhairiMarshall,SimonMoxon,ErikL.L.Sonnhammer,DavidJ.Studholme,CorinYeats,SeanR.Eddy", "Abstract": ["Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allow-ing Pfam domain de\u00aenitions to be closer to those found in structure databases. Pfam is available on the web in the UK"], "doi": "10.1.1.1020.771", "Keywords": [], "Labels": "database"},
{"title": ["Querying object-oriented databases  (1992) "], "Author": "MichaelKifer,WonKim,YehoshuaSagiv", "Abstract": ["We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92]."], "doi": "10.1.1.50.9598", "Keywords": ["object-oriented database", "database schema", "ffl database schema", "complete account", "expressive form", "path expression", "object-oriented language", "expressive power", "ffl view", "store schema information", "salient feature", "ffl precise model-theoretic semantics", "type correctness", "novel language", "first-order formalization", "extended path expression", "uniform way", "expository purpose", "relational language", "space limitation", "different circumstance", "wide variety"], "Labels": "database"},
{"title": ["Artificial Intelligence "], "Author": "StefanRank,PaoloPetta,\u00d6sterreichischesForschungsinstitutF\u00fcr,StefanRank,PaoloPetta,StefanRank,PaoloPetta", "Abstract": ["Basing artificial emotion on process and resource management The Austrian Research Institute for Artificial Intelligence is supported by the"], "doi": "10.1.1.130.4535", "Keywords": ["artificial intelligence", "artificial emotion", "austrian research institute", "resource management"], "Labels": "artificial intelligence"},
{"title": ["Adaptive clustering for mobile wireless networks  (1997) "], "Author": "ChunhungRichardLin,MarioGerla", "Abstract": ["This paper describes a self-organizing, multihop, mobile radio network, which relies on a code division access scheme for multimedia support. In the proposed network architecture, nodes are organized into nonoverlapping clusters. The clusters are independently controlled and are dynamically reconfigured as nodes move. This network architecture has three main advantages. First, it provides spatial reuse of the bandwidth due to node clustering. Secondly, bandwidth can be shared or reserved in a controlled fashion in each cluster. Finally, the cluster algorithm is robust in the face of topological changes caused by node motion, node failure and node insertion/removal. Simulation shows that this architecture provides an efficient, stable infrastructure for the integration of different types of traffic in a dynamic radio network. 1."], "doi": "10.1.1.100.5145", "Keywords": ["adaptive clustering", "mobile wireless network", "network architecture", "cluster algorithm", "main advantage", "stable infrastructure", "node failure", "different type", "mobile radio network", "controlled fashion", "node motion", "code division access scheme", "topological change", "multimedia support", "node insertion removal", "spatial reuse", "dynamic radio network"], "Labels": "multimedia"},
{"title": ["A Measurement Study of Peer-to-Peer File Sharing Systems  (2002) "], "Author": "StefanSaroiu,P.KrishnaGummadi,StevenD.Gribble", "Abstract": ["The popularity of peer-to-peer multimedia file sharing applications such as Gnutella and Napster has created a flurry of recent research activity into peer-to-peer architectures. We believe that the proper evaluation of a peer-to-peer system must take into account the characteristics of the peers that choose to participate. Surprisingly, however, few of the peer-to-peer architectures currently being developed are evaluated with respect to such considerations. In this paper, we remedy this situation by performing a detailed measurement study of the two popular peer-to-peer file sharing systems, namely Napster and Gnutella. In particular, our measurement study seeks to precisely characterize the population of end-user hosts that participate in these two systems. This characterization includes the bottleneck bandwidths between these hosts and the Internet at large, IP-level latencies to send packets to these hosts, how often hosts connect and disconnect from the system, how many les hosts share and download, the degree of cooperation between the hosts, and several correlations between these characteristics. Our measurements show that there is significant heterogeneity and lack of cooperation across peers participating in these systems."], "doi": "10.1.1.18.6611", "Keywords": ["measurement study", "peer-to-peer file", "peer-to-peer architecture", "end-user host", "bottleneck bandwidth", "popular peer-to-peer file", "peer-to-peer system", "significant heterogeneity", "ip-level latency", "proper evaluation", "detailed measurement study", "many le host share", "peer-to-peer multimedia file", "several correlation", "recent research activity"], "Labels": "multimedia"},
{"title": ["QuickTime VR - An Image-Based Approach to Virtual Environment Navigation  (1995) "], "Author": "ShenchangEricChen", "Abstract": ["Traditionally, virtual reality systems use 3D computer graphics to model and render virtual environments in real-time. This approach usually requires laborious modeling and expensive special purpose rendering hardware. The rendering quality and scene complexity are often limited because of the real-time constraint. This paper presents a new approach which uses 360-degree cylindrical panoramic images to compose a virtual environment. The panoramic image is digitally warped on-the-fly to simulate camera panning and zooming. The panoramic images can be created with computer rendering, specialized panoramic cameras or by \"stitching\" together overlapping photographs taken with a regular camera. Walking in a space is currently accomplished by \"hopping\" to different panoramic points. The image-based approach has been used in the commercial product QuickTime VR, a virtual reality extension to Apple Computer's QuickTime digital multimedia framework. The paper describes the architecture, the fil..."], "doi": "10.1.1.39.3043", "Keywords": ["image-based approach", "quicktime vr", "virtual environment navigation", "panoramic image", "virtual environment", "regular camera", "expensive special purpose", "rendering quality", "virtual reality system", "specialized panoramic camera", "quicktime digital multimedia framework", "apple computer", "computer rendering", "computer graphic", "virtual reality extension", "scene complexity", "real-time constraint", "different panoramic point", "warped on-the-fly", "commercial product quicktime vr", "laborious modeling", "360-degree cylindrical panoramic image", "new approach", "camera panning"], "Labels": "multimedia"},
{"title": ["Federated database systems for managing distributed, heterogeneous, and autonomous databases  (1990) "], "Author": "AmitP.Sheth,JamesA.Larson", "Abstract": ["A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS."], "doi": "10.1.1.118.6768", "Keywords": ["federated database system", "autonomous database", "popular architecture", "critical issue", "various fdbs architecture", "schema viewpoint", "distributed database management system", "database system", "reference architecture"], "Labels": "database"},
{"title": ["Equation-based congestion control for unicast applications  (2000) "], "Author": "SallyFloyd,MarkHandley,JitendraPadhye,J\u00f6rgWidmer", "Abstract": ["This paper proposes a mechanism for equation-based congestion control for unicast traffic. Most best-effort traffic in the current Internet is well-served by the dominant transport protocol, TCP. However, traffic such as best-effort unicast streaming multimedia could find use for a TCP-friendly congestion control mechanism that refrains from reducing the sending rate in half in response to a single packet drop. With our mechanism, the sender explicitly adjusts its sending rate as a function of the measured rate of loss events, where a loss event consists of one or more packets dropped within a single round-trip time. We use both simulations and experiments over the Internet to explore performance. We consider equation-based congestion control a promising avenue of development for congestion control of multicast traffic, and so an additional motivation for this work is to lay a sound basis for the further development of multicast congestion control."], "doi": "10.1.1.136.4408", "Keywords": ["equation-based congestion control", "unicast application", "loss event", "tcp-friendly congestion control mechanism", "sound basis", "additional motivation", "single round-trip time", "best-effort unicast streaming multimedia", "multicast traffic", "current internet", "measured rate", "multicast congestion control", "dominant transport protocol", "congestion control", "unicast traffic", "promising avenue", "single packet drop", "best-effort traffic"], "Labels": "multimedia"},
{"title": ["Data Streams: Algorithms and Applications  (2005) "], "Author": "S.Muthukrishnan", "Abstract": ["In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time, and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Researchers in Theoretical Computer Science, Databases, IP Networking and Computer Systems are working on the data stream challenges. This article is an overview and survey of data stream algorithmics and is an updated version of [175].1"], "doi": "10.1.1.418.1173", "Keywords": ["data stream", "input size", "ip networking", "new theory", "limited memory", "ip network traffic analysis", "data stream algorithmics", "theoretical computer science", "data stream challenge", "text message stream", "computer system", "data stream scenario", "communication complexity", "past year", "input arrives", "massive data set", "metric embeddings", "updated version", "pseudo-random computation", "sparse approximation theory"], "Labels": "algorithm theory"},
{"title": ["Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms  (2002) "], "Author": "MichaelCollins", "Abstract": ["We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."], "doi": "10.1.1.18.6725", "Keywords": ["perceptron algorithm", "discriminative training method", "hidden markov model", "base noun phrase chunking", "part-of-speech tagging", "classification problem", "conditional random field", "maximum-entropy model", "viterbi decoding", "new algorithm", "simple additive update", "experimental result", "training example", "maximum-entropy tagger"], "Labels": "algorithm theory"},
{"title": ["Ontology Learning for the Semantic Web  (2001) "], "Author": "ErMaedche,SteffenStaab", "Abstract": ["The Semantic Web relies heavily on the formal ontologies that structure underlying data for the purpose of comprehensive and transportable machine understanding. Therefore, the success of the Semantic Web depends strongly on the proliferation of ontologies, which requires fast and easy engineering of ontologies and avoidance of a knowledge acquisition bottleneck. Ontology Learning greatly facilitates the construction of ontologies by the ontology engineer. The vision of ontology learning that we propose here includes a number of complementary disciplines that feed on different types of unstructured, semi-structured and fully structured data in order to support a semi-automatic, cooperative ontology engineering process. Our ontology learning framework proceeds through ontology import, extraction, pruning, refinement, and evaluation giving the ontology engineer a wealth of coordinated tools for ontology modeling. Besides of the general framework and architecture, we show in this paper some exemplary techniques in the ontology learning cycle that we have implemented in our ontology learning environment, Text-To-Onto, such as ontology learning from free text, from dictionaries, or from legacy ontologies, and refer to some others that need to complement the complete architecture, such as reverse engineering"], "doi": "10.1.1.73.4559", "Keywords": ["semantic web", "ontology learning", "exemplary technique", "general framework", "free text", "easy engineering", "complementary discipline", "knowledge acquisition bottleneck", "different type", "ontology import", "transportable machine understanding", "cooperative ontology engineering process", "reverse engineering", "ontology modeling", "complete architecture", "formal ontology", "ontology engineer", "coordinated tool", "legacy ontology", "framework proceeds"], "Labels": "semantic web"},
{"title": ["Natural Language Processing  (2007) "], "Author": "CohnTrevorA,TrevorA.Cohn", "Abstract": ["Scaling conditional random fields for natural language processing Terms and Conditions: Terms and Conditions: Copyright in works deposited in Minerva Access is retained by the"], "doi": "10.1.1.905.8483", "Keywords": [], "Labels": "natural language processing"},
{"title": ["Linguistics and Natural Language Processing  (1987) "], "Author": "VictorRaskin", "Abstract": ["The paper addresses the issue of cooperation between linguistics and natural language processing (NLP), in general, and between linguistics and machine translation (MT), in particular. It focuses on just one direction of such cooperation, namely applications of linguistics to NLP, virtually ignoring"], "doi": "10.1.1.463.8510", "Keywords": ["natural language processing", "machine translation"], "Labels": "natural language processing"},
{"title": ["An Empirical Study of Operating System Errors  (2001) "], "Author": "AndyChou,JunfengYang,BenjaminChelf,SethHallem,DawsonEngler", "Abstract": ["We present a study of operating system errors found by automatic, static, compiler analysis applied to the Linux and OpenBSD kernels. Our approach differs from previ-ous studies that consider errors found by manual inspec-tion of logs, testing, and surveys because static analysis is applied uniformly to the entire kernel source, though our approach necessarily considers a less comprehensive variety of errors than previous studies. In addition, au-tomation allows us to track errors over multiple versions of the kernel source to estimate how long errors remain in the system before they are fixed. We found that device drivers have error rates up to three to seven times higher than the rest of the ker-nel. We found that the largest quartile of functions have error rates two to six times higher than the small-est quartile. We found that the newest quartile of files have error rates up to twice that of the oldest quartile, which provides evidence that code &quot;hardens &quot; over time. Finally, we found that bugs remain in the Linux kernel an average of 1.8 years before being fixed. 1"], "doi": "10.1.1.112.8742", "Keywords": ["operating system error", "empirical study", "multiple version", "approach differs", "au-tomation allows", "previous study", "entire kernel source", "small-est quartile", "comprehensive variety", "compiler analysis", "code quot", "long error", "static analysis", "previ-ous study", "kernel source", "openbsd kernel", "manual inspec-tion", "device driver"], "Labels": "operating systems"},
{"title": ["Natural Language Processing  (2001) "], "Author": "EnricoFranconi", "Abstract": ["In most natural language processing applications, Description Logics have been used to encode in a knowledge base some syntactic, semantic, and pragmatic elements needed to drive the semantic interpretation and the natural language generation processes. More recently, Description Logics have been used to fully characterise the semantic issues involved in the interpretation phase. In this Chapter the various proposals appeared in the literature about the use of Description Logics for natural language processing will be analysed.  15.1 "], "doi": "10.1.1.21.6514", "Keywords": ["natural language processing", "description logic", "natural language processing application", "semantic issue", "pragmatic element", "natural language generation", "various proposal", "semantic interpretation", "knowledge base", "interpretation phase"], "Labels": "natural language processing"},
{"title": ["Computer Vision: Algorithms and Applications  (2010) "], "Author": "RichardSzeliski", "Abstract": [" "], "doi": "10.1.1.393.8971", "Keywords": ["computer vision"], "Labels": "computer vision"},
{"title": ["The sprite network operating system  (1988) "], "Author": "JohnK.Ousterhout,AndrewR.Cherenson,FrederickDouglis,MichaelN.Nelson,BrentB.Welch", "Abstract": ["Sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. It implements a set of kernel calls much like those of 4.3 BSD UNIX, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. The implementation of the Sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix tables to implement a single file name space and to provide flexibility in administering the network file system, and large variable-size file caches on both client and server machines, which provide high performance even for diskless workstations."], "doi": "10.1.1.136.4159", "Keywords": ["sprite network", "server machine", "remote procedure call facility", "several interesting feature", "prefix table", "new operating system", "bsd unix", "multiprocessor workstation", "networked uniprocessor", "share memory", "network file system", "high performance", "large variable-size file cache", "sprite kernel", "large physical memory", "diskless workstation", "single file name space"], "Labels": "operating systems"},
{"title": ["Real-Time Dynamic Voltage Scaling for Low-Power Embedded Operating Systems  (2001) "], "Author": "PadmanabhanPillai,KangG.Shin", "Abstract": ["In recent years, there has been a rapid and wide spread of nontraditional computing platforms, especially mobile and portable computing devices. As applications become increasingly sophisticated and processing power increases, the most serious limitation on these devices is the available battery life. Dynamic Voltage Scaling (DVS) has been a key technique in exploiting the hardware characteristics of processors to reduce energy dissipation by lowering the supply voltage and operating frequency. The DVS algorithms are shown to be able to make dramatic energy savings while providing the necessary peak computation power in general-purpose systems. However, for a large class of applications in embedded real-time systems like cellular phones and camcorders, the variable operating frequency interferes with their deadline guarantee mechanisms, and DVS in this context, despite its growing importance, is largely overlooked/under-developed. To provide real-time guarantees, DVS must consider deadlines and periodicity of real-time tasks, requiring integration with the real-time scheduler. In this paper, we present a class of novel algorithms called real-time DVS (RT-DVS) that modify the OS's real-time scheduler and task management service to provide significant energy savings while maintaining real-time deadline guarantees. We show through simulations and a working prototype implementation that these RT-DVS algorithms closely approach the theoretical lower bound on energy consumption, and can easily reduce energy consumption 20% to 40% in an embedded real-time system."], "doi": "10.1.1.11.5713", "Keywords": ["real-time dynamic voltage", "low-power embedded operating system", "energy consumption", "real-time scheduler", "available battery life", "real-time dvs", "variable operating frequency", "hardware characteristic", "wide spread", "power increase", "serious limitation", "large class", "operating frequency", "energy dissipation", "real-time task", "dvs algorithm", "cellular phone", "key technique", "general-purpose system", "necessary peak computation power", "embedded real-time system", "rt-dvs algorithm", "dynamic voltage scaling", "supply voltage", "significant energy saving", "novel algorithm", "real-time deadline guarantee", "task management service", "nontraditional computing platform", "working prototype implementation", "recent year", "real-time guarantee", "deadline guarantee mechanism", "dramatic energy saving", "real-time system"], "Labels": "operating systems"},
{"title": [" Marketing in Hypermedia Computer-Mediated Environments: Conceptual Foundations  (1995) "], "Author": "DonnaL.Hoffman,ThomasP.Novak", "Abstract": [" "], "doi": "10.1.1.195.3875", "Keywords": ["conceptual foundation", "hypermedia computer-mediated environment"], "Labels": "computer education"},
{"title": ["Basic concepts and taxonomy of dependable and secure computing  (2004) "], "Author": "AlgirdasAvizienis,Jean-claudeLaprie,BrianRandell,CarlLandwehr", "Abstract": [" This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures."], "doi": "10.1.1.219.5446", "Keywords": ["basic concept", "secure computing", "system failure", "particular type", "main definition", "general concept", "fault forecasting", "additional definition", "fault removal", "technical community", "basic definition", "fault prevention", "special case attribute", "wide range", "generic concept", "fault tolerance"], "Labels": "computer education"},
{"title": ["Human Computer Interaction "], "Author": "DanaSpiegel", "Abstract": ["This paper discusses the research that has been done in the field of Human Computer Interaction (HCI) relating to human psychology. A brief overview of HCI is presented. Specific examples of research in the areas of icons and menus are then reviewed. The results of these experiments and the predictions they make about general human psychology and specific human interaction with computers is discussed. Mental models of user interface interaction are discussed and compared to mental models of real world object. Finally, future directions for research are proposed. 2"], "doi": "10.1.1.169.6314", "Keywords": ["human computer interaction", "mental model", "future direction", "real world object", "specific example", "general human psychology", "brief overview", "specific human interaction", "human psychology", "user interface interaction"], "Labels": "human computer interaction"},
{"title": ["THE GROWTH OF HUMAN-COMPUTER INTERACTION "], "Author": "TerryWinograd", "Abstract": ["This paper is the closing address for CHI \u201890. It addresses the problem of educating computer professionals in the area of human-computer interaction, arguing that standard approaches within computer science need to be augmented and that new models of education can aid us in producing students with broad competence in the design of computer systems for human use."], "doi": "10.1.1.651.339", "Keywords": ["human-computer interaction", "broad competence", "standard approach", "computer professional", "computer system", "computer science need", "new model", "human use", "closing address"], "Labels": "human computer interaction"},
{"title": ["Computing Inequality: Have Computers Changed the Labor Market?  (1998) "], "Author": "DavidH.Autor,LawrenceF.Katz,AlanB.Krueger", "Abstract": ["This paper examines the effect of skill-biased technological change as measured by computerization on the recent widening of U. S. educational wage differentials. An analysis of aggregate changes in the relative supplies and wages of workers by education from 1940 to 1996 indicates strong and persistent growth in relative demand favoring college graduates. Rapid skill upgrading within detailed industries accounts for most of the growth in the relative demand for college workers, particularly since 1970. Analyses of four data sets indicate that the rate of skill upgrading has been greater in more computer-intensive industries.  "], "doi": "10.1.1.308.459", "Keywords": [], "Labels": "computer education"},
{"title": ["An Introduction to Human Computer Interaction "], "Author": "MikeSharples", "Abstract": ["Introduction to Human Computer Interaction  Mike Sharples  School of Cognitive and Computing Sciences  University of Sussex  mike@cogs.susx.ac.uk  1. Interacting with Computers  1.1. Background to HCI  Human-computer interaction (HCI) is the study of the ways people interact with and through computers. It grew out of work on human factors (the U.S. term) or ergonomics (the European term) concerned with the design and usability of equipment. The aim of the early work in HCI was to provide interfaces to computer programs which hid the low-level operations of the computer and allowed the user to work through direct interaction with familiar entities such as words and diagrams, rather than by writing detailed instructions in a computer language. More recently the scope of HCI has broadened to include interface design, formal models of users and their interactions with computers, evaluation of computer systems, computer-mediated human to human communication an"], "doi": "10.1.1.37.158", "Keywords": ["human computer interaction", "computing science university", "direct interaction", "human communication", "familiar entity", "computer language", "interface design", "way people", "hci human-computer interaction", "european term", "low-level operation", "detailed instruction", "u.s. term", "human factor", "computer system", "early work", "computer program", "computer-mediated human", "formal model"], "Labels": "human computer interaction"},
{"title": ["Human-Computer Interaction?\u201d "], "Author": "LarsErikHolmquist", "Abstract": ["www.viktoria.se/fal When seeing the title of this workshop, I realize that in academia, we are often fixated on the computer itself rather than on what it is supposed to accomplish. Our conferences are called things like \u201cHuman-Computer Interaction\u201d, \u201cUbiquitous Computing\u201d, or \u201cComputer Graphics\u201d. Even supposedly alternative paradigms like \u201cambient \u201d or \u201cpervasive \u201d computing places too much emphasis on how things are done, rather than what we should be doing. In fact, the notion of \u201chuman-computer interaction \u201d is a remnant of an increasingly outmoded way of thinking about digital technology. The term \u201ccomputer \u201d itself is now so loaded with meaning, that looking for new ways to \u201cinteract with computers \u201d can be genuinely counterproductive. Instead of looking for new human-computer interfaces, what we should think about is interaction models and applications where humans and computing (rather than computers) co-exist. I propose that rather than \u201chuman-computer interaction\u201d, we should talk about \u201cthe use and design of digital artefacts.\u201d Let\u2019s take a few steps back. During the last decade, HCI researchers have been developing a multitude of alternatives to the traditional desktop computer. In the hope of improving how we"], "doi": "10.1.1.135.8465", "Keywords": ["human-computer interaction", "alternative paradigm", "interaction model", "se fal", "ubiquitous computing", "digital artefact", "new human-computer interface", "traditional desktop computer", "last decade", "term computer", "much emphasis", "pervasive computing place", "hci researcher", "digital technology", "computer graphic", "new way", "outmoded way"], "Labels": "human computer interaction"},
{"title": ["Applications of Intelligent Agents  (1998) "], "Author": "N.R.Jennings,M.Wooldridge", "Abstract": [" "], "doi": "10.1.1.39.6599", "Keywords": ["intelligent agent"], "Labels": "intelligent agents"},
{"title": ["JACK Intelligent Agents - Components for Intelligent Agents in Java  (1999) "], "Author": "PaoloBusetta,RalphRonnquist,AndrewHodgson,AndrewLucas", "Abstract": ["This paper is organised as follows. Section 2 introduces JACK Intelligent Agents, presenting the approach taken by AOS to its design and outlining its major engineering characteristics. The BDI model is discussed briefly in Section 3. Section 4 gives an outline of how to build an application with JACK Intelligent Agents. Finally, in Section 5 we discuss how the use of this framework can be beneficial to both engineers and researchers. For brevity, we will refer to JACK Intelligent Agents simply as \"JACK\"."], "doi": "10.1.1.30.6936", "Keywords": ["intelligent agent", "jack intelligent agent component", "jack intelligent agent", "bdi model", "introduces jack intelligent agent", "major engineering characteristic"], "Labels": "intelligent agents"},
{"title": ["Distributed Intelligent Agents  (1996) "], "Author": "KatiaSycara,KeithDecker,AnandeepPannu,MikeWilliamson,DajunZeng", "Abstract": ["We are investigating techniques for developing distributed and adaptive collections of agents that coordinate to retrieve, filter and fuse information relevant to the user, task and situation, as well as anticipate a user's information needs. In our system of agents, information gathering is seamlessly integrated with decision support. The task for which particular information is requested of the agents does not remain in the user's head but it is explicitly represented and supported through agent collaboration. In this paper we present the distributed system architecture, agent collaboration interactions, and a reusable set of software components for constructing agents. We call this reusable multi-agent computational infrastructure RETSINA (Reusable Task Structure-based Intelligent Network Agents). It has three types of agents. Interface agents interact with the user receiving user specifications and delivering results. They acquire, model, and utilize user preferences to guide syste..."], "doi": "10.1.1.36.7025", "Keywords": ["intelligent agent", "decision support", "adaptive collection", "reusable set", "agent collaboration interaction", "software component", "fuse information relevant", "user preference", "user specification", "particular information", "distributed system architecture", "information gathering", "interface agent", "reusable multi-agent computational infrastructure retsina", "information need", "agent collaboration"], "Labels": "intelligent agents"},
{"title": [" FUTURE PATHS FOR INTEGER PROGRAMMING AND LINKS TO Artificial Intelligence  (1986) "], "Author": "FredGlover", "Abstract": ["Scope and Purpose-A summary is provided of some of the recent (and a few not-so-recent) developments that otTer promise for enhancing our ability to solve combinatorial optimization problems. These developments may be usefully viewed as a synthesis of the perspectives of operations research and artificial intelligence. Although compatible with the use of algorithmic subroutines, the frameworks examined are primarily heuristic, based on the supposition that etTective solution of complex combinatorial structures in some cases may require a level of flexibility beyond that attainable by methods with formally demonstrable convergence properties. Abstract-Integer programming has benefited from many innovations in models and methods. Some of the promising directions for elaborating these innovations in the future may be viewed from a framework that links the perspectives of artificial intelligence and operations research. To demonstrate this, four key areas are examined: (1) controlled randomization, (2) learning strategies, (3) induced decomposition and (4) tabu search. Each of these is shown to have characteristics that appear usefully relevant to developments on the horizon.  "], "doi": "10.1.1.90.7851", "Keywords": ["operation research", "artificial intelligence", "many innovation", "promising direction", "algorithmic subroutine", "complex combinatorial structure", "purpose-a summary", "abstract-integer programming", "combinatorial optimization problem", "key area", "demonstrable convergence property", "ettective solution", "learning strategy", "tabu search"], "Labels": "artificial intelligence"},
{"title": ["ARTIFICIAL INTELLIGENCE "], "Author": "RavneetKaur,ChahalRavneet,KaurAmanbirKaur", "Abstract": ["Artificial intelligence is a human endeavor to create a non-organic machine-based entity, that has all the above abilities of natural organic intelligence Researchers are creating systems which can mimic human thought, understand speech, beat the best human chessplayer, and countless other feats never before possible. This paper describes about the various approaches and applications of Artificial Intelligence"], "doi": "10.1.1.673.9145", "Keywords": ["artificial intelligence", "human chessplayer", "natural organic intelligence researcher", "human endeavor", "various approach", "human thought", "understand speech", "non-organic machine-based entity", "countless feat"], "Labels": "artificial intelligence"},
{"title": ["Epistemological Problems of Artificial Intelligence  (1977) "], "Author": "JohnMccarthy", "Abstract": ["In (McCarthy and Hayes 1969), we proposed dividing the artificial intelligence problem into two parts\u2014an epistemological part and a heuristic part. This lecture further explains this division, explains some of the epistemological problems, and presents some new results and approaches."], "doi": "10.1.1.398.4288", "Keywords": ["epistemological problem", "artificial intelligence", "heuristic part", "epistemological part", "new result", "artificial intelligence problem"], "Labels": "artificial intelligence"},
{"title": ["Steps toward artificial intelligence  (1961) "], "Author": "MarvinMinsky", "Abstract": ["Harvard University. The work toward attaining &quot;artificial intelligence\u2019 \u2019 is the center of considerable computer research, design, and application. The field is in its starting transient, characterized by many varied and independent efforts. Marvin Minsky has been requested to draw this work together into a coherent summary, supplement it with appropriate explanatory or theoretical noncomputer information, and introduce his assessment of the state of the art. This paper emphasizes the class of activities in which a general-purpose computer, complete with a library of basic programs, is further programmed to perform operations leading to ever higher-level information processing functions such as learning and problem solving. This informative article will be of real interest to both the general Proceedings reader and the computer specialist.-- The Guest Editor."], "doi": "10.1.1.79.7413", "Keywords": ["artificial intelligence", "guest editor", "independent effort", "marvin minsky", "general-purpose computer", "harvard university", "considerable computer research", "appropriate explanatory", "general proceeding reader", "coherent summary", "many varied", "higher-level information", "informative article", "starting transient", "theoretical noncomputer information", "problem solving", "computer specialist", "basic program", "real interest"], "Labels": "artificial intelligence"},
{"title": ["Data Mining: Concepts and Techniques  (2000) "], "Author": "JiaweiHan,MichelineKamber", "Abstract": ["Our capabilities of both generating and collecting data have been increasing rapidly in the last several decades. Contributing factors include the widespread use of bar codes for most commercial products, the computerization of many business, scientific and government transactions and managements, and advances in data collection tools ranging from scanned texture and image platforms, to on-line instrumentation in manufacturing and shopping, and to satellite remote sensing systems. In addition, popular use of the World Wide Web as a global information system has flooded us with a tremendous amount of data and information. This explosive growth in stored data has generated an urgent need for new techniques and automated tools that can intelligently assist us in transforming the vast amounts of data into useful information and knowledge. This book explores the concepts and techniques of data mining, a promising and flourishing frontier in database systems and new database applications. Data mining, also popularly referred to as knowledge discovery in databases (KDD), is the automated or convenient extraction of patterns representing knowledge implicitly stored in large databases, data warehouses, and other massive information repositories. Data mining is a multidisciplinary field, drawing work from areas including database technology, artificial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge based systems, knowledge acquisition, information retrieval, high performance computing, and data visualization. We present the material in"], "doi": "10.1.1.118.1099", "Keywords": ["data mining", "global information system", "useful information", "information retrieval", "many business", "tremendous amount", "neural network", "image platform", "vast amount", "urgent need", "database system", "knowledge acquisition", "data collection tool", "government transaction", "artificial intelligence", "widespread use", "new technique", "database technology", "automated tool", "data warehouse", "new database application", "multidisciplinary field", "popular use", "explosive growth", "on-line instrumentation", "world wide web", "pattern recognition", "data visualization", "bar code", "high performance computing", "machine learning", "knowledge discovery", "large database", "convenient extraction", "commercial product", "massive information repository", "last several decade"], "Labels": "data mining"},
{"title": [" \t Privacy-Preserving Data Mining    (2000) "], "Author": "RakeshAgrawal,RamakrishnanSrikant", "Abstract": ["A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from tredning data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a-novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data. "], "doi": "10.1.1.131.1561", "Keywords": ["privacy-preserving data mining", "individual data record", "a-novel reconstruction procedure", "data record", "following question", "future data mining research", "decision-tree classifier", "data mining", "original data value", "original data", "original distribution", "primary task", "individual record", "original record", "privacy concern", "original value", "data value", "concrete case", "accurate model", "fruitful direction"], "Labels": "data mining"},
{"title": ["Artificial Intelligence: "], "Author": "HypeOr", "Abstract": ["M ost definitions of artificial intelligence in the standard textsare overly complex for a general survey of the field, so hereis a simple one that will suffice instead: Artificial intelli-gence is the science of mimicking human mental facultiesin a computer. Pinpointing the beginning of AI research is tricky. George Boole (1815-1864) had plenty of ideas on the mathematical analysis of thought processes, and the field still retains several of his ideas. However, since he had no com-puter, my simple definition rules out Boole as AI\u2019s founder. Just as historians on either side of the Atlantic have different opinions about who built the first programmable computer, they also diverge over AI\u2019s origins. British historians point to Alan Turing\u2019s 1950 article that defined what is now known as the Turing test for determining whether a computer displays intelligence.1 American historians prefer to point to the Dartmouth conference of 1956, which was explicitly billed as a study of AI and is believed to have been the source of the term \u201cartificial intelligence.\u201d A SPECTRUM OF INTELLIGENT BEHAVIOR A difficulty with my simple definition of AI is that it leaves the notion of intelligence rather vague. Figure 1 helps clarify the matter by showing a spectrum of intelligent behaviors based on the level of understanding involved. The lowest-level behaviors include instinctive reactions, such as withdrawing a hand from a hot object or dodging a projectile. High-level behaviors demand specialist expertise such as in the legal requirements of company takeovers or the interpretation of mass spectrograms. Researchers have developed conventional computing techniques to han-dle the low-level decision making and control needed for the low end of the spectrum. Highly effective computer systems exist for monitoring and con-trolling a variety of equipment. For example, Figure 2 shows the Advanced Step in Innovative Mobility robot"], "doi": "10.1.1.708.399", "Keywords": [], "Labels": "artificial intelligence"},
{"title": ["Artificial Intelligence "], "Author": "R.J.Sternberg,S.B.Kaufman(editors,AshokK.Goel,JimDavies", "Abstract": ["Artificial intelligence (AI) is the field of research that strives to understand, design and build cognitive systems. From computer programs that can beat top international grand masters at chess to robots that can help detect improvised explosive devices in war, AI has had many successes. As a science, it differs"], "doi": "10.1.1.673.3320", "Keywords": ["artificial intelligence", "cognitive system", "top international grand master", "improvised explosive device", "computer program", "many success"], "Labels": "artificial intelligence"},
{"title": ["Efficient similarity search in sequence databases  (1994) "], "Author": "RakeshAgrawal,ChristosFaloutsos,ArunSwami", "Abstract": ["We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval's theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coe cients, we use R-trees to index the sequences and e ciently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1-3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences. "], "doi": "10.1.1.219.5983", "Keywords": ["sequence database", "efficient similarity search", "similarity query", "frequency domain", "time sequence", "crucial observation", "discrete fourier transform", "good performance", "fourier transform", "sequential scanning", "practical interest", "indexing method", "important observation", "euclidean distance", "lower-dimensionality space", "performance gain", "experimental result", "first frequency", "first fourier coe cients"], "Labels": "database"},
{"title": ["Comprehensive database for facial expression analysis "], "Author": "TakeoKanade,JeffreyF.Cohn,YingliTian", "Abstract": ["Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expression, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity, image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive test-bed to date for comparative studies of facial expression analysis. 1."], "doi": "10.1.1.173.220", "Keywords": ["facial expression analysis", "comprehensive database", "scene complexity", "digitized image sequence", "image characteristic", "test data", "limited data set", "significant effort", "head orientation", "individual difference", "multiple token", "various method", "primary facs action unit", "non-verbal behavior", "problem space", "past decade", "comparative study", "adult subject"], "Labels": "database"},
{"title": ["Query evaluation techniques for large databases  (1993) "], "Author": "GoetzGraefe", "Abstract": ["Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today\u2019s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains."], "doi": "10.1.1.108.3178", "Keywords": ["evaluation technique", "large database", "large set", "wide array", "post-relational database system", "modern data model", "complex query evaluation plan", "database management system", "acceptable performance", "query execution facility", "iterative execution", "practical query evaluation technique", "simple record", "parallel query execution", "new database management system", "efficient algorithm", "database system", "special operator", "extensible database system", "hash-based set", "architectural issue", "complex object", "database application domain", "large data volume", "database management software", "solid understanding", "query processing algorithm"], "Labels": "database"},
{"title": ["Parallel database systems: the future of high performance database systems  (1992) "], "Author": "DavidJ.Dewitt,JimGray", "Abstract": ["Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1."], "doi": "10.1.1.104.8594", "Keywords": ["parallel database system", "high performance database system", "research system", "impressive speedup", "exotic hardware", "parallel database machine architecture", "relational database query", "dataflow architecture", "conventional shared-nothing hardware", "new design"], "Labels": "database"},
{"title": ["Imagenet: A large-scale hierarchical image database  (2009) "], "Author": "JiaDeng,WeiDong,RichardSocher,Li-jiaLi,KaiLi,LiFei-fei", "Abstract": ["The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond. 1."], "doi": "10.1.1.155.1729", "Keywords": ["large-scale hierarchical image database", "critical problem", "wordnet structure", "image data", "semantic hierarchy", "image classification", "large-scale database", "computer vision community", "multimedia data", "largescale ontology", "amazon mechanical turk", "data collection scheme", "new database", "unparalleled opportunity", "full resolution image", "object recognition", "automatic object", "annotated image", "current state", "simple application", "detailed analysis", "robust model", "challenging task", "hierarchical structure", "current image datasets", "500-1000 clean"], "Labels": "multimedia"},
{"title": ["Statistical pattern recognition: A review  (2000) "], "Author": "AnilK.Jain,RobertP.W.Duin,JianchangMao", "Abstract": ["The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have bean receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."], "doi": "10.1.1.123.8151", "Keywords": ["statistical pattern recognition", "pattern recognition", "statistical approach", "arbitrary orientation", "review paper", "feature extraction", "recognition system", "data mining", "research topic", "following issue", "web searching", "various stage", "test sample", "multimedia data", "challenging field", "pattern class", "neural network technique", "general problem", "various framework", "pattern recognition system", "unsupervised classification", "performance evaluation", "careful attention", "complex pattern", "face recognition", "efficient pattern recognition technique", "well-known method", "cursive handwriting recognition", "pattern representation", "primary goal", "cluster analysis"], "Labels": "multimedia"},
{"title": ["A Comparative Analysis of Methodologies for Database Schema Integration  (1986) "], "Author": "C.Batini,M.Lenzerini,S.B.Navathe", "Abstract": [" One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries. \r\n\r\nMethodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema. The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions."], "doi": "10.1.1.107.7214", "Keywords": [], "Labels": "database"},
{"title": ["Adapting Golog for composition of semantic web Services  (2002) "], "Author": "SheilaMcilraith", "Abstract": ["Motivated by the problem of automatically composing network accessible services, such as those on the World Wide Web, this paper proposes an approach to building agent technology based on the notion of generic procedures and customizing user constraint. We argue that an augmented version of the logic programming language Golog provides a natural formalism for automatically composing services on the Semantic Web. To this end, we adapt and extend the Golog language to enable programs that are generic, customizable and usable in the context of the Web. Further, we propose logical criteria for these generic procedures that define when they are knowledge self-sufficient and physically selfsufficient. To support information gathering combined with search, we propose a middle-ground Golog interpreter that operates under an assumption of reasonable persistence of certain information. These contributions are realized in our augmentation of a ConGolog interpreter that combines online execution of information-providing Web services with offline simulation of worldaltering Web services, to determine a sequence of Web Services for subsequent execution. Our implemented system is currently interacting with services on the Web. 1"], "doi": "10.1.1.105.7433", "Keywords": ["semantic web service", "web service", "generic procedure", "certain information", "golog language", "information gathering", "semantic web", "natural formalism", "online execution", "reasonable persistence", "logic programming language golog", "information-providing web service", "user constraint", "agent technology", "offline simulation", "subsequent execution", "middle-ground golog interpreter", "network accessible service", "world wide web", "implemented system", "logical criterion", "congolog interpreter", "augmented version"], "Labels": "semantic web"},
{"title": ["Agents and the Semantic Web  (2001) "], "Author": "JamesHendler", "Abstract": ["Many challenges of bringing communicating multiagent systems to the Web require ontologies. The integration of agent technology and ontologies could significantly affect the use of Web services and the ability to extend programs to perform tasks for users more efficiently and with less human intervention."], "doi": "10.1.1.11.6264", "Keywords": ["semantic web", "web service", "agent technology", "many challenge", "multiagent system", "web require ontology", "human intervention"], "Labels": "semantic web"},
{"title": ["Trust management for the semantic web  (2003) "], "Author": "MatthewRichardson,RakeshAgrawal,PedroDomingos", "Abstract": ["Abstract. Though research on the Semantic Web has progressed at a steady pace, its promise has yet to be realized. One major difficulty is that, by its very nature, the Semantic Web is a large, uncensored system to which anyone may contribute. This raises the question of how much credence to give each source. We cannot expect each user to know the trustworthiness of each source, nor would we want to assign top-down or global credibility values due to the subjective nature of trust. We tackle this problem by employing a web of trust, in which each user provides personal trust values for a small number of other users. We compose these trusts to compute the trust a user should place in any other user in the network. A user is not assigned a single trust rank. Instead, different users may have different trust values for the same user. We define properties for combination functions which merge such trusts, and define a class of functions for which merging may be done locally while maintaining these properties. We give examples of specific functions and apply them to data from Epinions and our BibServ bibliography server. Experiments confirm that the methods are robust to noise, and do not put unreasonable expectations on users. We hope that these methods will help move the Semantic Web closer to fulfilling its promise. 1."], "doi": "10.1.1.297.8900", "Keywords": ["semantic web", "trust management", "specific function", "semantic web closer", "single trust rank", "uncensored system", "bibserv bibliography server", "personal trust value", "major difficulty", "though research", "unreasonable expectation", "steady pace", "subjective nature", "combination function", "different trust value", "small number", "much credence", "different user", "global credibility value"], "Labels": "semantic web"},
{"title": ["Semantic Web "], "Author": "MajdiBeseiso,AbdulRahimAhmad,RoslanIsmail", "Abstract": ["Information availability is a key factor in the acquisition of knowledge. Access to information either in the general area or even in more specific ones like sciences, languages, and religion become wider since the use of semantics in World Wide Web. Semantic Web technologies assist in the acquiring of information by creating processes that link information to another. However, the technology supports mostly languages using Latin family scripts. Arabic is still not well supported. This paper, reports on the survey of the support for Arabic in some of the existing Semantic Web technologies, and give future scenario in applying Semantic Web for Arabic applications. Finally, multilingual support for these new technologies is also discussed."], "doi": "10.1.1.206.5459", "Keywords": ["semantic web", "semantic web technology", "key factor", "world wide web", "religion become wider", "information availability", "specific one", "future scenario", "latin family script", "new technology", "general area", "arabic application", "multilingual support"], "Labels": "semantic web"},
{"title": ["Semantic Web "], "Author": "GilesHogben", "Abstract": ["A privacy enhancing identity management framework using the semantic web Wspomag\u0105jaca prywatno\u015b\u0107 struktura ramowa zarz\u0105dzania to\u017csamo\u015bci\u0105 z wykorzystaniem"], "doi": "10.1.1.163.1086", "Keywords": [], "Labels": "semantic web"},
{"title": ["Tutorial on Natural Language Processing "], "Author": "UnknownAuthors", "Abstract": ["Natural languages are languages spoken by humans. Currently we are not yet at the point where these languages in all of their unprocessed forms can be understood by computers. Natural language processing is the collection of techniques employed to try and accomplish that goal. The field of natural language processing (NLP) is deep and diverse, This paper will introduce natural language understanding and generation to the reader then go in depth on how these topics work and relate to NLP as a whole. Furthermore, this paper will discuss the applications and challenges of NLP, namely duplicate error report detection, tutoring systems, and database interfaces. 1."], "doi": "10.1.1.100.4660", "Keywords": ["natural language processing", "database interface", "topic work", "unprocessed form", "natural language understanding", "duplicate error report detection", "natural language"], "Labels": "natural language processing"},
{"title": ["Natural Language Processing/Robotics "], "Author": "TimothyBrick", "Abstract": ["Robots that interact with humans face-to-face using natural language need to be responsive to the way humans use language in those situations. We propose a psychologicallyinspired natural language processing system for robots which performs incremental semantic interpretation of spoken utterances, integrating tightly with the robot\u2019s perceptual and motor systems."], "doi": "10.1.1.91.9793", "Keywords": ["natural language processing robotics", "incremental semantic interpretation", "motor system", "natural language need", "spoken utterance", "way human", "psychologicallyinspired natural language processing system"], "Labels": "natural language processing"},
{"title": ["Statistical Natural Language Processing  (2003) "], "Author": "AliFarghaly,ChrisCallison-Burch,MilesOsborne", "Abstract": ["Introduction  Statistical natural language processing (SNLP)    is a field lying in the intersection of natural language processing and machine learning. SNLP di#ers from traditional natural language processing in that instead of having a linguist manually construct some model of a given linguistic phenomenon, that model is instead (semi-) automatically constructed from linguistically annotated resources. Methods for assigning partof -speech tags to words, categories to texts, parse trees to sentences, and so on, are (semi-) automatically acquired using machine learning techniques.  The recent trend of applying statistical techniques to natural language processing came largely from industrial speech recognition research groups at AT&T's Bell Laboratories and IBM's T.J. Watson Research Center. Statistical techniques in speech recognition have so vastly outstripped the performance of their non-statistical counterparts that rule-based speech recognition systems are essentially no longer a"], "doi": "10.1.1.13.6279", "Keywords": ["statistical natural language processing", "natural language processing", "statistical technique", "machine learning", "traditional natural language processing", "introduction statistical natural language processing", "annotated resource", "recent trend", "non-statistical counterpart", "rule-based speech recognition system", "snlp di er", "speech recognition", "industrial speech recognition research group", "t.j. watson research center", "bell laboratory", "linguistic phenomenon", "partof speech tag"], "Labels": "natural language processing"},
{"title": ["Improving the reliability of commodity operating systems  (2003) "], "Author": "MichaelM.Swift,BrianN.Bershad,HenryM.Levy", "Abstract": ["drivers remain a significant cause of system failures. In Windows XP, for example, drivers account for 85 % of recently reported failures. This article describes Nooks, a reliability subsystem that seeks to greatly enhance operating system (OS) reliability by isolating the OS from driver failures. The Nooks approach is practical: rather than guaranteeing complete fault tolerance through a new (and incompatible) OS or driver architecture, our goal is to prevent the vast majority of driver-caused crashes with little or no change to the existing driver and system code. Nooks isolates drivers within lightweight protection domains inside the kernel address space, where hardware and software prevent them from corrupting the kernel. Nooks also tracks a driver\u2019s use of kernel resources to facilitate automatic cleanup during recovery. To prove the viability of our approach, we implemented Nooks in the Linux operating system and used it to fault-isolate several device drivers. Our results show that Nooks offers a substantial increase in the reliability of operating systems, catching and quickly recovering from many faults that would otherwise crash the system. Under a wide range and number of fault conditions, we show that Nooks recovers automatically from 99 % of the faults that otherwise cause Linux to crash."], "doi": "10.1.1.107.2596", "Keywords": ["kernel address space", "automatic cleanup", "fault condition", "fault-isolate several device driver", "driver architecture", "system code", "linux operating system", "window xp", "reliability subsystem", "many fault", "complete fault tolerance", "driver use", "vast majority", "wide range", "significant cause", "substantial increase", "kernel resource", "nook recovers", "operating system", "driver failure", "lightweight protection domain", "nook approach", "driver-caused crash", "system failure"], "Labels": "operating systems"},
{"title": [" An Overview of the C++ Programming Language  (1999) "], "Author": "BjarneStroustrup", "Abstract": ["This overview of C++ presents the key design, programming, and language-technical concepts using examples to give the reader a feel for the language. C++ is a general-purpose programming language with a bias towards systems programming that supports efficient low-level computation, data abstraction, object-oriented programming, and generic programming.  "], "doi": "10.1.1.217.3477", "Keywords": ["programming language", "data abstraction", "bias towards system", "language-technical concept", "key design", "generic programming", "efficient low-level computation", "object-oriented programming", "general-purpose programming language"], "Labels": "programming languages"},
{"title": ["CASIMIRO: A Robot Head for Human-Computer Interaction "], "Author": "ModestoCastrill\u00f3nSantana,JavierLorenzo-navarro,DanielHern\u00e1ndez-sosa,SeeProfile,J.Lorenzo,C.Guerra,PalmasGranCanaria", "Abstract": ["CASIMIRO:\t a\trobot\thead\tfor\thuman-computer interaction"], "doi": "10.1.1.885.4502", "Keywords": [], "Labels": "human computer interaction"},
{"title": ["Information and Computer Education, "], "Author": "Jung-yingLai,NationalKaohsiung,Chun-fangLin,NationalKaohsiung,Chung-huangYang,NationalKaohsiung", "Abstract": ["Information and Computer Education,"], "doi": "10.1.1.173.7018", "Keywords": ["computer education"], "Labels": "computer education"},
{"title": ["Intelligent Agent  (1997) "], "Author": "TechnologyChaired,StephenRank,MrsJones", "Abstract": ["ends (Alexandra Coddington) who used to be at U.C.L. with Maria Fox as her Ph.D. supervisor. At about 10 a.m. we were ushered into the conference room. Dr Greenough from R.A.L. said a few words about the fire alarms (get outside if there's a continuous bell) and other alarms (a klaxon sounding means that the nearby Harwell Nuclear Reactor is in the process of meltdown. The advice given is this happened was to get inside and hide under a table). If both the fire alarm (meaning `get out') and the klaxon (meaning `stay inside') sound, Dr Greenough seemed to think that the advice was to find a substantial building. 2 The Attendees  We were given an attendance list on arrival, along with a program. The people attending were divided into the following categories; Chairman 1 Speakers 5 Organisers 2 Participants 49 About 60% of the `Participants' were from universities, a couple of people were from the research council and the rest from `industry'. 3 The Meeting  "], "doi": "10.1.1.47.4155", "Keywords": ["intelligent agent", "dr greenough", "fire alarm", "attendance list", "continuous bell", "substantial building", "alexandra coddington", "research council", "maria fox", "klaxon sounding", "following category", "nearby harwell nuclear reactor", "conference room"], "Labels": "intelligent agents"},
{"title": ["Intelligent Agents "], "Author": "CentreForComp,AnaPaiva,HciCentre,ArnavJhala", "Abstract": ["Abstract \u2014 Confronting conflicts and coping with them is part of social life, since conflicts seem to arise in almost every context and developmental stage of human life. The personal and collective gains that follow conflict resolution have motivated scholars across many research fields to advocate the use of pro-social mechanisms for resolution. The Siren serious game aims to support teachers \u2019 role to educate young people on how to resolve conflicts, by employing affect-aware, userand cultural adaptivity to provide interesting and relevant conflict scenarios and resolution approaches."], "doi": "10.1.1.299.5492", "Keywords": [], "Labels": "intelligent agents"},
{"title": ["Universal artificial intelligence "], "Author": "MarcusHutter", "Abstract": [" "], "doi": "10.1.1.636.8947", "Keywords": ["intelligent agent", "universal ai", "ultimate super-intelligence", "previous system", "build general-purpose super-intelligences", "general ai", "gold standard", "various implication", "biological organism", "computational aspect", "last century", "agi research", "artificial general intelligence", "crucial aspect", "intelligent algorithm", "detonation cord", "machine intelligence", "formal definition", "artificial intelligence", "philosophical mathematical", "various approach", "first theory", "th thi sys", "australian national university", "brief review", "heuristic consideration", "future challenge"], "Labels": "intelligent agents"},
{"title": ["Intelligent Agents  (1997) "], "Author": "ErPivk,Matja\u017eGams", "Abstract": ["Intelligent agents have been applied to electronic commerce, promising a revolution in the way we conduct business, whether business-to-business, business-to-customer or customer-to-customer. This article gives a brief review of agent technologies involved in buying and selling, followed by lists of Internet e-commerce agents. Several agent-mediated electronic commerce systems are analysed in the context of a general model of the buying process. Several lists of related Internet links should help readers to gather additional relevant information. We presented an intelligent employment agent and ecommerce related agent modules in the system. Our experience indicates that new agent capabilities offer advanced functions in e-commerce. 1"], "doi": "10.1.1.97.7438", "Keywords": ["intelligent agent", "new agent capability", "electronic commerce", "several agent-mediated electronic commerce system", "ecommerce related agent module", "related internet link", "several list", "intelligent employment agent", "agent technology", "internet e-commerce agent", "brief review", "buying process", "general model", "additional relevant information", "advanced function"], "Labels": "intelligent agents"},
{"title": [" Data Mining Approaches for Intrusion Detection "], "Author": "WenkeLee,SalvatoreJ.Stolfo", "Abstract": ["In this paper we discuss our research in developing general and systematic methods for intrusion detection. The key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. Using experiments on the sendmail system call data and the network tcpdump data, we demonstrate that we can construct concise and accurate classifiers to detect anomalies. We provide an overview on two general data mining algorithms that we have implemented: the association rules algorithm and the frequent episodes algorithm. These algorithms can be used to compute the intra- and inter- audit record patterns, which are essential in describing program or user behavior. The discovered patterns can guide the audit data gathering process and facilitate feature selection. To meet the challenges of both efficient learning (mining) and real-time detection, we propose an agent-based architecture for intrusion detection systems where the learning agents continuously compute and provide the updated (detection) models to the detection agents. "], "doi": "10.1.1.134.4855", "Keywords": ["intrusion detection", "data mining approach", "user behavior", "detection agent", "systematic method", "network tcpdump data", "frequent episode", "sendmail system call data", "accurate classifier", "learning agent", "relevant system feature", "useful pattern", "intrusion detection system", "key idea", "discovered pattern", "inter audit record pattern", "agent-based architecture", "audit data gathering process", "facilitate feature selection", "efficient learning", "system feature", "general data mining algorithm", "real-time detection", "association rule"], "Labels": "data mining"},
{"title": ["Intelligent Agents  (1997) "], "Author": "AnibalMorales-Morell", "Abstract": ["Software Agents are the new revolution in software usability. Agents are described with anthropomorphic features such as intelligence, autonomy, and such. The school of researchers from the fields of Artificial Intelligence (AI), Distributed AI (DAI), and other more pragmatic researchers are engaged in defining these \"software entities\" and creating this new field of study and development along the way. In the mean time, while researchers fight for formal definitions, this new technology is bringing powerful and useful tools to the end user, in particular managing huge amount of data, such as that produced by the Internet, and taking decisions on what the user may be interested in. 1. Introduction  `Intelligent Agent' and the more generic term `Agent' are terms that have been used interchangeably, since some kind of \"intelligence\" has been consistently described as one of the attributes of a software agent [Franklin, 96]. One of the definitions of software agents describes them as \"cha..."], "doi": "10.1.1.56.8576", "Keywords": ["intelligent agent", "software agent", "huge amount", "software agent franklin", "mean time", "pragmatic researcher", "software entity", "new revolution", "useful tool", "new field", "new technology", "anthropomorphic feature", "software usability", "end user", "formal definition", "artificial intelligence", "generic term"], "Labels": "intelligent agents"},
{"title": ["Data Mining: An Overview from Database Perspective  (1996) "], "Author": "Ming-syanChen,JiaweiHun,PhilipS.Yu", "Abstract": ["Mining information and knowledge from large databases has been recognized by many researchers  as a key research topic in database systems and machine learning, and by many  industrial companies as an important area with an opportunity of major revenues. Researchers  in many different fields have shown great interest in data mining. Several emerging applications  in information providing services, such as data warehousing and on-line services over the  Internet, also call for various data mining techniques to better understand user behavior, to  improve the service provided, and to increase the business opportunities. In response to such  a demand, this article is to provide a survey, from a database researcher's point of view, on  the data mining techniques developed recently. A classification of the available data mining  techniques is provided and a comparative study of such techniques is presented."], "doi": "10.1.1.12.7980", "Keywords": ["data mining", "database perspective", "database researcher", "business opportunity", "data mining technique", "on-line service", "great interest", "many industrial company", "machine learning", "user behavior", "database system", "various data mining technique", "many different field", "key research topic", "information providing service", "many researcher", "available data mining technique", "major revenue", "important area", "comparative study", "large database"], "Labels": "data mining"},
{"title": ["From data mining to knowledge discovery in databases  (1996) "], "Author": "UsamaFayyad,GregoryPiatetsky-shapiro,PadhraicSmyth", "Abstract": ["\u25a0 Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are"], "doi": "10.1.1.116.3382", "Keywords": ["knowledge discovery", "data mining", "medium attention", "specific data-mining technique", "wide variety", "article mention particular real-world application", "machine learning", "significant amount", "future research direction", "real-world application"], "Labels": "data mining"},
{"title": ["Efficient and Effective Clustering Methods for Spatial Data Mining  (1994) "], "Author": "RaymondT.Ng,JiaweiHan", "Abstract": ["Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also de- velop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms."], "doi": "10.1.1.13.4395", "Keywords": ["spatial data mining", "effective clustering method", "interesting relationship", "new clustering method", "spatial database", "spatial data mining algorithm", "randomized search", "current spatial data mining algorithm", "use clarans"], "Labels": "data mining"},
{"title": ["The WEKA Data Mining Software: An Update "], "Author": "MarkHall,EibeFrank,GeoffreyHolmes,BernhardPfahringer,PeterReutemann,IanH.Witten", "Abstract": ["More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003. 1."], "doi": "10.1.1.148.3671", "Keywords": ["weka data mining software", "stable release", "widespread acceptance", "weka workbench", "active community", "briefly discus", "last stable version", "first public release"], "Labels": "data mining"},
{"title": [" Problems with semantic Web "], "Author": "WeekBMicroformats", "Abstract": ["introduction to semantic web, agents."], "doi": "10.1.1.643.2728", "Keywords": ["semantic web"], "Labels": "semantic web"},
{"title": ["The Semantic Web and . . .  "], "Author": "GuntherEysenbach", "Abstract": ["The `Semantic Web' can be thought of an extension of the present web, as an additional machine-processable layer of data beneath the visible layer of human-readable information. In the first part of this chapter I will briefly review the building blocks of the Semantic Web, such as metadata expressed in the format of the Resource Description Framework (RDF). In the second part, I will provide some examples and review the prospects of the Semantic Web for the field of knowledge management and knowledge translation in consumer health informatics; for example, supporting decisions to be made by consumers, for improving access to information, and for addressing questions around the quality of health information on the web. Perhaps the most significant application of the Semantic Web for the health field is trust management, i.e. helping consumers to identify high quality trustworthy health resources on the web."], "doi": "10.1.1.69.1618", "Keywords": ["semantic web", "second part", "first part", "visible layer", "trust management", "knowledge translation", "knowledge management", "additional machine-processable layer", "consumer health informatics", "significant application", "health resource", "resource description framework", "health field", "high quality", "human-readable information", "present web", "building block", "health information"], "Labels": "semantic web"},
{"title": ["Description Logics for the Semantic Web  (2001) "], "Author": "FranzBaader,IanHorrocks,UlrikeSattler", "Abstract": ["The vision of a Semantic Web has recently drawn considerable attention,  both from academia and industry. Description Logics are often  named as one of the tools that can support the Semantic Web and thus  help to make this vision reality."], "doi": "10.1.1.13.8693", "Keywords": ["semantic web", "description logic", "considerable attention", "vision reality"], "Labels": "semantic web"},
{"title": ["Trust networks on the semantic Web  (2003) "], "Author": "JenniferGolbeck,BijanParsia,JamesHendler", "Abstract": ["Abstract. The so-called \"Web of Trust \" is one of the ultimate goals of the Semantic Web. Research on the topic of trust in this domain has focused largely on digital signatures, certificates, and authentication. At the same time, there is a wealth of research into trust and social networks in the physical world. In this paper, we describe an approach for integrating the two to build a web of trust in a more social respect. This paper describes the applicability of social network analysis to the semantic web, particularly discussing the multi-dimensional networks that evolve from ontological trust specifications. As a demonstration of algorithms used to infer trust relationships, we present several tools that allow users to take advantage of trust metrics that use the network. 1"], "doi": "10.1.1.295.6894", "Keywords": [], "Labels": "semantic web"},
{"title": ["Natural Language Processing "], "Author": "AbhimanyuChopra,AbhinavPrashar,ReshSain", "Abstract": ["ABSTRACT: Language is way of communicating your words Language helps in understanding the world,we get a better insight of the world. Language helps speakers to be as vague or as precise as they like. NLP Stands for natural language processing.. Natural languages are those languages that are spoken by the people.Natural language processing girdles everything a computer needs to understand natural language and also generates natural language.Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics mainly focuses on the interactions between computers and human languages or natural languages. NLP is focussed on the area of human computer interaction. The need for natural language processing was also felt because there is a wide storage of information recorded or stored in natural language that could be accessible via computers. Information is constantly generated in the form of books, news, business and government reports, and scientific papers, many of which are available online or even in some reports. A system requiring a great deal of information must be able to process natural language to retrieve much of the information available on computers. Natural language processing is an interesting and difficult field in which we have to develop and evaluate or analyse representation and reasoning theories. All of the problems of AI arise in this domain; solving \"the natural language problem \" is as difficult as solving \"the AI problem \" because any field can be expressed or can be depicted in natural language."], "doi": "10.1.1.407.6907", "Keywords": ["natural language processing", "natural language", "human computer interaction", "scientific paper", "word language", "wide storage", "natural language processing girdle everything", "difficult field", "nlp stand", "human language", "natural language problem", "ai arise", "computer science", "government report", "ai problem", "great deal", "available online", "artificial intelligence"], "Labels": "natural language processing"},
{"title": ["Natural language processing of lyrics  (2005) "], "Author": "JoseP.G.Mahedero", "Abstract": ["We report experiments on the use of standard natural language processing (NLP) tools for the analysis of music lyrics. A significant amount of music audio has lyrics. Lyrics encode an important part of the semantics of a song, therefore their analysis complements that of acoustic and cultural metadata and is fundamental for the development of complete music information retrieval systems. Moreover, a textual analysis of a song can generate ground truth data that can be used to validate results from purely acoustic methods. Preliminary results on language identification, structure extraction, categorization and similarity searches suggests that a lot of profit can be gained from the analysis of lyrics."], "doi": "10.1.1.74.3046", "Keywords": ["natural language processing", "acoustic method", "important part", "textual analysis", "complete music information retrieval system", "language identification", "structure extraction", "standard natural language processing", "significant amount", "similarity search", "music lyric", "cultural metadata", "music audio", "ground truth data", "analysis complement", "preliminary result"], "Labels": "natural language processing"},
{"title": ["Natural language processing  (1996) "], "Author": "MarkSteedman,LincLab,MarkSteedman,MarkSteedman", "Abstract": ["Natural Language Processing The subject of Natural Language Processing can be considered in both broad and narrow senses. In the broad sense, it covers processing issues at all levels of natural language understanding, including speech recognition, syntactic and semantic analysis of sentences, reference to the discourse context (including anaphora, inference of referents, and more extended relations of discourse coherence and narrative structure), conversational inference and implicature, and discourse planning and generation. In the narrower sense, it covers the syntactic and semantic processing sentences to deliver semantic objects suitable for referring, inferring, and the like. Of course, the results of inference and reference may under some circumstances play a part in processing in the narrow sense. But the processes that are characteristic of these other modules are not the primary"], "doi": "10.1.1.650.6553", "Keywords": ["natural language processing", "speech recognition", "natural language", "narrow sens", "discourse coherence", "semantic analysis", "broad sense", "narrative structure", "narrow sense", "extended relation", "semantic object", "semantic processing sentence", "conversational inference", "natural language understanding", "discourse planning", "discourse context"], "Labels": "natural language processing"},
{"title": ["The C Programming Language  (1988) "], "Author": "DennisM.Ritchie", "Abstract": ["The C programming language was devised in the early 1970s as a system implementation language for the nascent Unix operating system. Derived from the typeless language BCPL, it evolved a type structure; created on a tiny machine as a tool to improve a meager programming environment, it has become one of the dominant languages of today. This paper studies its evolution."], "doi": "10.1.1.177.9578", "Keywords": ["programming language", "type structure", "dominant language", "system implementation language", "typeless language bcpl", "paper study", "tiny machine", "nascent unix"], "Labels": "programming languages"},
{"title": ["  Information Retrieval   "], "Author": "VijayV.Raghavan,etal.", "Abstract": [], "doi": "10.1.1.127.91", "Keywords": ["information retrieval"], "Labels": "information retrieval"},
{"title": ["Information Retrieval  (1979) "], "Author": "C.J.vanRijsbergen", "Abstract": [" "], "doi": "10.1.1.36.2325", "Keywords": ["information retrieval"], "Labels": "information retrieval"},
{"title": ["Distributed Intelligent Agents for Control,  (1994) "], "Author": "DiagnosisAndRepair,WolfgangNejdl,MartinWerner,InformatikV", "Abstract": ["In this paper we discuss the use of distributed intelligent agents for  building a system incorporating the tasks control, diagnosis and repair."], "doi": "10.1.1.9.9045", "Keywords": ["intelligent agent", "distributed intelligent agent", "task control"], "Labels": "intelligent agents"},
{"title": ["The Semantics of Predicate Logic as a Programming Language  (1976) "], "Author": "M.H.VanEmden,R.A.Kowalski", "Abstract": ["ABSTRACT Sentences in first-order predicate logic can be usefully interpreted as programs In this paper the operational and fixpomt semantics of predicate logic programs are defined, and the connections with the proof theory and model theory of logic are investigated It is concluded that operational semantics is a part of proof theory and that fixpolnt semantics is a special case of model-theoret:c semantics KEY WORDS AND PHRASES predicate logic as a programming language, semantics of programming languages, resolution theorem proving, operaUonal versus denotatlonal semantics, SL-resoluuon, flxpomt characteriza-tion"], "doi": "10.1.1.64.9246", "Keywords": ["programming language", "predicate logic", "proof theory", "first-order predicate logic", "operauonal versus denotatlonal semantics", "operational semantics", "fixpolnt semantics", "abstract sentence", "fixpomt semantics", "semantics key word phrase", "model theory", "special case", "predicate logic program", "resolution theorem proving"], "Labels": "programming languages"},
{"title": ["The synchronous dataflow programming language LUSTRE  (1991) "], "Author": "N.Halbwachs,P.Caspi,P.Raymond,D.Pilaud", "Abstract": ["This paper describes the language Lustre, which is a dataflow synchronous language, designed for programming reactive systems --- such as automatic control and monitoring systems --- as well as for describing hardware. The dataflow aspect of Lustre makes it very close to usual description tools in these domains (block-diagrams, networks of operators, dynamical samples-systems, etc: : : ), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. Finally, the Lustre formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology. 1 Introduction Reactive systems Reactive systems have been defined as computing systems which continuously interact with a given physical environment, when this environment is unable to sy..."], "doi": "10.1.1.34.5059", "Keywords": ["synchronous dataflow programming language lustre", "synchronous interpretation", "dataflow synchronous language", "program property", "lustre formalism", "language lustre", "dataflow aspect", "efficient sequential program", "introduction reactive system reactive system", "usual description tool", "physical environment", "dynamical samples-systems", "temporal logic", "reactive system", "automatic control", "original program verification methodology"], "Labels": "programming languages"},
{"title": ["Program Analysis and Specialization for the C Programming Language  (1994) "], "Author": "LarsOleAndersen", "Abstract": ["Software engineers are faced with a dilemma. They want to write general and wellstructured programs that are flexible and easy to maintain. On the other hand, generality has a price: efficiency. A specialized program solving a particular problem is often significantly faster than a general program. However, the development of specialized software is time-consuming, and is likely to exceed the production of today\u2019s programmers. New techniques are required to solve this so-called software crisis. Partial evaluation is a program specialization technique that reconciles the benefits of generality with efficiency. This thesis presents an automatic partial evaluator for the Ansi C programming language. The content of this thesis is analysis and transformation of C programs. We develop several analyses that support the transformation of a program into its generating extension. A generating extension is a program that produces specialized programs when executed on parts of the input. The thesis contains the following main results."], "doi": "10.1.1.109.6502", "Keywords": ["program analysis", "programming language", "specialized program", "new technique", "generating extension", "today programmer", "specialized software", "automatic partial evaluator", "software engineer", "main result", "general program", "program specialization technique", "particular problem", "partial evaluation", "several analysis", "wellstructured program", "so-called software crisis"], "Labels": "programming languages"},
{"title": ["Survey of clustering data mining techniques  (2002) "], "Author": "PavelBerkhin", "Abstract": ["Accrue Software, Inc. Clustering is a division of data into groups of similar objects. Representing the data by fewer clusters necessarily loses certain fine details, but achieves simplification. It models data by its clusters. Data modeling puts clustering in a historical perspective rooted in mathematics, statistics, and numerical analysis. From a machine learning perspective clusters correspond to hidden patterns, the search for clusters is unsupervised learning, and the resulting system represents a data concept. From a practical perspective clustering plays an outstanding role in data mining applications such as scientific data exploration, information retrieval and text mining, spatial database applications, Web analysis, CRM, marketing, medical diagnostics, computational biology, and many others. Clustering is the subject of active research in several fields such as statistics, pattern recognition, and machine learning. This survey focuses on clustering in data mining. Data mining adds to clustering the complications of very large datasets with very many attributes of different types. This imposes unique"], "doi": "10.1.1.145.895", "Keywords": ["data mining technique", "data mining", "numerical analysis", "many attribute", "unsupervised learning", "outstanding role", "information retrieval", "accrue software", "computational biology", "spatial database application", "historical perspective", "large datasets", "many others", "model data", "text mining", "data modeling", "active research", "similar object", "scientific data exploration", "practical perspective clustering", "pattern recognition", "different type", "perspective cluster", "medical diagnostics", "machine learning", "data concept", "certain fine detail", "web analysis", "several field"], "Labels": "data mining"},
{"title": ["The Elements of Statistical Learning --  Data Mining, Inference, and Prediction "], "Author": "TrevorHastie,RobertTibshirani,JeromeFriedman", "Abstract": [" "], "doi": "10.1.1.229.864", "Keywords": ["prediction second editionthis", "data mining", "page printer", "page vii"], "Labels": "data mining"},
{"title": ["Hardware Architectures "], "Author": "SujinPhilip,BrianSumma,ValerioPascucci", "Abstract": ["\u25cf Clusters of commodity hardware are the most ubiquitous form of computing power available today. \u25cf The processing power of such clusters has been consistently increasing. \u25cf This performance increase is not only due to the increase in the number of connected nodes but also due to the increase in available parallelism within the nodes, in the form of multicore CPUs and GPUs. \u25cf Hybrid computing is the efficient use of all these different types of resources to achieve high performance."], "doi": "10.1.1.731.9662", "Keywords": [], "Labels": "hardware architecture"},
{"title": ["Private Information Retrieval    (1997) "], "Author": "BennyChor,OdedGoldreich,EyalKushilevitz,MadhuSudan", "Abstract": [" Publicly accessible databases are an indispensable resource for retrieving up to date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user's queries and infer what the user is after. Indeed, in cases where the users ' intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be downloaded, namely n bits should be communicated (where n is the number of bits in the database). In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be obtained. We describe schemes that enable a user to access k replicated copies of a database (k * 2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we have ffl A two database scheme with communication complexity of O(n1=3). ffl A scheme for a constant number, k, of databases with communication complexity O(n1=k). ffl A scheme for 13 log2 n databases with polylogarithmic (in n) communication complexity."], "doi": "10.1.1.126.9441", "Keywords": ["private information retrieval", "communication complexity", "constant number", "log2 database", "database scheme", "individual database", "single database", "private retrieval problem", "date information", "indispensable resource", "curious database operator", "efficient solution", "whole database", "publicly accessible database", "significant risk"], "Labels": "information retrieval"},
{"title": ["Modern Information Retrieval  (1999) "], "Author": "RicardoBaeza-Yates,BerthierRibeiro-Neto", "Abstract": ["Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book"], "doi": "10.1.1.27.7690", "Keywords": ["modern information retrieval", "information retrieval", "complete way", "world wide web", "computer-science perspective", "inexpensive graphical user interface", "user-centered perspective", "mass storage device", "new ir book", "first course", "last year", "great need", "graduate course", "traditional ir textbook"], "Labels": "information retrieval"},
{"title": ["GOLOG: A Logic Programming Language for Dynamic Domains  (1994) "], "Author": "HectorJ.Levesque,RaymondReiter,YvesLesp\u00e9rance,FangzhenLin,RichardB.Scherl", "Abstract": ["This paper proposes a new logic programming language called GOLOG whose interpreter automatically maintains an explicit representation of the dynamic world being modeled, on the basis of user supplied axioms about the preconditions and effects of actions and the initial state of the world. This allows programs to reason about the state of the world and consider the effects of various possible courses of action before committing to a particular behavior. The net effect is that programs may be written at a much higher level of abstraction than is usually possible. The language appears well suited for applications in high level control of robots and industrial processes, intelligent software agents, discrete event simulation, etc. It is based on a formal theory of action specified in an extended version of the situation calculus. A prototype implementation in Prolog has been developed."], "doi": "10.1.1.54.7045", "Keywords": ["dynamic domain", "logic programming language", "discrete event simulation", "explicit representation", "initial state", "formal theory", "situation calculus", "new logic programming language", "particular behavior", "extended version", "prototype implementation", "various possible course", "net effect", "high level control", "dynamic world", "industrial process", "intelligent software agent"], "Labels": "programming languages"},
{"title": ["The Esterel Synchronous Programming Language: Design, Semantics, Implementation  (1992) "], "Author": "GerardBerry,GeorgesGonthier", "Abstract": [" "], "doi": "10.1.1.17.5606", "Keywords": ["esterel synchronous programming language"], "Labels": "programming languages"},
{"title": ["Hardware Architecture for Protocol Processing "], "Author": "TomasHenriksson", "Abstract": ["iii Protocol processing is increasingly important. Through the years the hardware architectures for network equipment have evolved constantly. It is important to make a difference between terminals and routers and the different processing tasks they encounter. It is also important to analyze in detail the functional coverage of a hardware architecture. The maximal supported line speed is also interesting and especially which functionality can be kept at this line speed. There are some types of hardware architectures that have gained much attention in research and from industry. Among these application specific instruction set computers, RISC with optimized instruction sets and reconfigurable hardware architectures are most often used. Very many network processors have been presented that aim for routers. So far not many protocol processors for terminals have been suggested. In terminals the requirements are different, for example low power consumption is very important for battery powered terminals. I and my colleagues have proposed a novel way to build a protocol processor for a terminal. The main concept is to use an array of reconfigurable functional pages, which are connected in a deep"], "doi": "10.1.1.127.4705", "Keywords": ["hardware architecture", "protocol processing", "functional coverage", "reconfigurable hardware architecture", "main concept", "iii protocol processing", "many protocol processor", "different processing task", "many network processor", "line speed", "reconfigurable functional page", "novel way", "network equipment", "maximal supported line speed", "protocol processor", "much attention", "optimized instruction set", "application specific instruction", "example low power consumption"], "Labels": "hardware architecture"},
{"title": ["Energy-efficient communication protocol for wireless microsensor networks  (2000) "], "Author": "WendiRabinerHeinzelman,AnanthaChandrakasan,HariBalakrishnan", "Abstract": ["Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. "], "doi": "10.1.1.112.3914", "Keywords": ["wireless microsensor network", "energy-efficient communication protocol", "energy dissipation", "sensor network", "dynamic network", "energy load", "military application", "overall energy dissipation", "low-energy adaptive clustering hierarchy", "useful system lifetime", "direct transmission", "multihop routing", "static clustering", "significant impact", "conventional protocol", "base station", "microsensor system", "communication protocol", "conventional routing protocol", "data fusion", "randomized rotation", "reliable monitoring", "clustering-based protocol", "local cluster base station"], "Labels": "network communication"},
{"title": ["Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval  (1998) "], "Author": "DavidD.Lewis", "Abstract": ["The naive Bayes classifier, currently experiencing a renaissance  in machine learning, has long been a core technique in information  retrieval. We review some of the variations of naive Bayes models used for  text retrieval and classification, focusing on the distributional assump-  tions made about word occurrences in documents."], "doi": "10.1.1.11.8264", "Keywords": ["information retrieval", "independence assumption", "word occurrence", "naive bayes model", "naive bayes classifier", "core technique", "text retrieval", "distributional assump tions", "machine learning"], "Labels": "information retrieval"},
{"title": ["Deconstructing hardware architectures for security  (2006) "], "Author": "MichaelDalton,HariKannan,ChristosKozyrakis", "Abstract": ["Researchers have recently proposed novel hardware architectures for enhancing system security. The proposed architectures address security threats such as buffer overflows, format string bugs, and information disclosure. The main advantage of hardware support is increased visibility into system state, low overheads for security checks, and, in some cases, compatibility with legacy binaries. Nevertheless, hardware support is not a panacea for system security. We review two architectures for preventing memory corruption and two for preventing information leaks. We identify significant vulnerabilities and shortcomings in these designs. We also discuss solutions and mitigation strategies. 1."], "doi": "10.1.1.125.2165", "Keywords": ["hardware architecture", "hardware support", "system security", "main advantage", "mitigation strategy", "information leak", "novel hardware", "format string bug", "significant vulnerability", "system state", "legacy binary", "proposed architecture address security threat", "low overhead", "memory corruption", "security check", "information disclosure"], "Labels": "hardware architecture"},
{"title": ["Hardware Architectures for Mobile Robots "], "Author": "LeopoldoArmesto,JuanCarlos,TorresJosepTornero", "Abstract": ["Abstract: This paper deals with the general problem of designing hardware architectures for mobile robots. In particular, four different architectures of mobile robots have been studied. An architecture for an industrial forklift, used in the automation of management and transport processes is firstly described. An architecture for a convoying application with an electric car is secondly showed. Finally, two RT platforms are described based on distributed architectures mainly using CAN field bus. The Robuter II and the PCBot II vehicles, have been used for research applications in these latest implementations."], "doi": "10.1.1.617.3139", "Keywords": ["mobile robot", "hardware architecture", "transport process", "rt platform", "robuter ii", "electric car", "pcbot ii vehicle", "industrial forklift", "distributed architecture", "different architecture", "convoying application", "research application", "general problem", "field bus"], "Labels": "hardware architecture"},
{"title": ["Using Linear Algebra for Intelligent Information Retrieval  (1995) "], "Author": "MichaelW.Berry,SusanT.Dumais", "Abstract": ["Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users' requests and those in or assigned to documents in a database. Because of the tremendous diversity in the words people use to describe the same document, lexical methods are necessarily incomplete and imprecise. Using the singular value decomposition (SVD), one can take advantage of the implicit higher-order structure in the association of terms with documents by determining the SVD of large sparse term by document matrices. Terms and documents represented by 200-300 of the largest singular vectors are then matched against user queries. We call this retrieval method Latent Semantic Indexing (LSI) because the subspace represents important associative relationships between terms and documents that are not evident in individual documents. LSI is a completely automatic yet intelligent indexing method, widely applicable, and a promising way to improve users..."], "doi": "10.1.1.33.3138", "Keywords": ["intelligent information retrieval", "linear algebra", "singular value decomposition", "singular vector", "important associative relationship", "word people", "individual document", "large sparse term", "user query", "implicit higher-order structure", "textual material", "document matrix", "lexical method", "retrieval method latent semantic indexing", "lexical match", "tremendous diversity", "promising way", "scientific database"], "Labels": "information retrieval"},
{"title": ["A Language Modeling Approach to Information Retrieval  (1998) "], "Author": "JayM.Ponte,W.BruceCroft", "Abstract": ["Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem. We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either. Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have..."], "doi": "10.1.1.54.6410", "Keywords": ["information retrieval", "document retrieval", "document indexing", "indexing model", "many retrieval model", "prior assumption", "adequate indexing model", "retrieval performance", "difficult problem", "integral part", "single model", "unwarranted parametric assumption", "several researcher", "collection statistic", "probabilistic language modeling"], "Labels": "information retrieval"},
{"title": ["Direct manipulation: a step beyond programming languages  (1983) "], "Author": "BenShneiderman", "Abstract": ["Direct manipulation systems offer the satisfying experience of operating on visible objects. The computer becomes transparent, and users can concentrate on their tasks."], "doi": "10.1.1.296.5944", "Keywords": ["direct manipulation", "satisfying experience", "direct manipulation system", "visible object"], "Labels": "programming languages"},
{"title": ["Model Checking for Programming Languages using VeriSoft  (1997) "], "Author": "PatriceGodefroid", "Abstract": ["Verification by state-space exploration, also often referred to as \"model checking\", is an effective method for analyzing the correctness of concurrent reactive systems (e.g., communication protocols). Unfortunately, existing model-checking techniques are restricted to the verification of properties of models, i.e., abstractions, of concurrent systems.  In this paper, we discuss how model checking can be extended to deal directly with \"actual\" descriptions of concurrent systems, e.g., implementations of communication protocols written in programming languages such as C or C++. We then introduce a new search technique that is suitable for exploring the state spaces of such systems. This algorithm has been implemented in VeriSoft, a tool for systematically exploring the state spaces of systems composed of several concurrent processes executing arbitrary C code. As an example of application, we describe how VeriSoft successfully discovered an error in a 2500-line C program controlling rob..."], "doi": "10.1.1.25.8581", "Keywords": ["model checking", "programming language", "state space", "concurrent system", "communication protocol", "state-space exploration", "several concurrent process", "model-checking technique", "concurrent reactive system", "new search technique", "2500-line program", "effective method", "actual description", "arbitrary code"], "Labels": "programming languages"},
{"title": ["On agent-based software engineering  (2000) "], "Author": "NicholasR.Jennings,MichaelWooldridge", "Abstract": ["Agent-oriented techniques represent an exciting new means of analysing, designing and building complex software systems. They have the potential to significantly improve current practice in software engineering and to extend the range of applications that can feasibly be tackled. Yet, to date, there have been few serious attempts to cast agent systems as a software engineering paradigm. This paper seeks to rectify this omission. Specifically, it will be argued that: (i) the conceptual apparatus of agent-oriented systems is well-suited to building software solutions for complex systems and (ii) agent-oriented approaches represent a genuine advance over the current state of the art for engineering complex systems. Following on from this view, the major issues raised by adopting an agent-oriented approach to software engineering are highlighted and discussed."], "doi": "10.1.1.101.5226", "Keywords": ["agent-based software engineering", "software engineering", "agent-oriented approach", "current practice", "serious attempt", "complex system", "software solution", "software engineering paradigm", "agent system", "genuine advance", "current state", "agent-oriented technique", "major issue", "building complex software system", "new mean", "engineering complex system", "agent-oriented system", "conceptual apparatus"], "Labels": "software engineering"},
{"title": ["Wireless Communications  (2005) "], "Author": "AndreaGoldsmith", "Abstract": [" "], "doi": "10.1.1.142.1045", "Keywords": ["wireless communication", "cambridge university press", "cambridge university", "relevant collective licensing agreement", "statutory exception"], "Labels": "network communication"},
{"title": ["Directed Diffusion: A scalable and robust communication paradigm for sensor networks  (2000) "], "Author": "ChalermekIntanagonwiwat,RameshGovindan,DeborahEstrin", "Abstract": ["Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network."], "doi": "10.1.1.117.1253", "Keywords": ["robust communication paradigm", "sensor network", "directed diffusion-based network", "directed diffusion", "distributed sensing", "radio technology", "data in-network", "environmental phenomenon", "good path", "energy saving", "cheap node", "directed diffusion paradigm", "simple remote-surveillance sensor network"], "Labels": "network communication"},
{"title": ["Hardware architecture Results and conclusion  (2010) "], "Author": "MultiplicationOverFp,NicolasGuillermin,LetM", "Abstract": ["i=1mi a set of coprime RNS form of X < M is {x1,..., xn} such that xi = |X |mi (1) X = n\u22121\u2211 i=0 (|xi.M\u22121i |mi)\u00d7Mi"], "doi": "10.1.1.642.1057", "Keywords": ["hardware architecture result", "coprime rn form"], "Labels": "hardware architecture"},
{"title": ["A Survey of Hardware Architectures Designed for Speech Recognition  (1991) "], "Author": "Hsiao-wuenHon,Hsiao-wuenHon", "Abstract": ["A survey of hardware architectures designed for speech recognition"], "doi": "10.1.1.1029.8844", "Keywords": [], "Labels": "hardware architecture"},
{"title": ["Generative communication in Linda  (1985) "], "Author": "DavidGelernter", "Abstract": ["Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires."], "doi": "10.1.1.113.9679", "Keywords": ["generative communication", "generative communication result", "integrated network computer", "new language", "previous interprocess communication model", "continuation passing", "process chooses", "programming langauge", "independent entity", "dynamic global name space", "distributed setting", "tuple-structured form", "generative communication model", "computation environment", "part ii", "novel implementation problem"], "Labels": "network communication"},
{"title": ["Information Retrieval as Statistical Translation  (1999) "], "Author": "AdamBerger,JohnLafferty", "Abstract": ["We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or \"translate\" a given document into a query. To assess the relevance of a document to a user's query, we estimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justification of the \"language modeling\" strategy recently proposed by Ponte and Croft. In a series of experiments on TREC data, a simple translation-based retrieval system performs well in compa..."], "doi": "10.1.1.43.7803", "Keywords": ["information retrieval", "statistical translation", "motivated model", "unsupervised manner", "statistical machine translation", "prior distribution", "simple translation-based retrieval system", "trec data", "document-to-query translation process", "new probabilistic approach", "general preference", "statistical model", "central ingredient"], "Labels": "information retrieval"},
{"title": ["Visual Information Retrieval  (1997) "], "Author": "AmarnathGupta,RetrievalAmarnathGupta,RameshJain", "Abstract": ["ND BUSINESSMAN CALVIN MOORES COINED the term information retrieval [10] to describe the process through which a prospective user of information can convert a request for information into a useful collection of references. \"Information retrieval,\" he wrote, \"embraces the intellectual aspects of the description of information and its specification for search, and also whatever systems, techniques, or machines that are employed Amarnath Gupta and Ramesh Jain 72 May 1997/Vol. 40, No. 5 COMMUNICATIONS OF THE ACM lar expressions to describe a clip. There is also a deeper reason: The information sought is inherently in the form of imagery that a textual language, however powerful, is unable to express adequately, making query processing inefficient. HE ROLE OF THE EMERGING FIELD OF visual information retrieval (VIR) systems is to go beyond text-based descri"], "doi": "10.1.1.36.3012", "Keywords": ["visual information retrieval", "ramesh jain", "amarnath gupta", "emerging field visual information retrieval", "intellectual aspect", "information retrieval", "text-based descri", "information sought", "query processing inefficient", "term information retrieval", "useful collection", "acm lar expression", "prospective user", "textual language", "nd businessman calvin moore coined"], "Labels": "information retrieval"},
{"title": ["Definitional interpreters for higher-order programming languages  (1972) "], "Author": "JohnC.Reynolds", "Abstract": ["Abstract. Higher-order programming languages (i.e., languages in which procedures or labels can occur as values) are usually defined by interpreters that are themselves written in a programming language based on the lambda calculus (i.e., an applicative language such as pure LISP). Examples include McCarthy\u2019s definition of LISP, Landin\u2019s SECD machine, the Vienna definition of PL/I, Reynolds \u2019 definitions of GEDANKEN, and recent unpublished work by L. Morris and C. Wadsworth. Such definitions can be classified according to whether the interpreter contains higher-order functions, and whether the order of application (i.e., call by value versus call by name) in the defined language depends upon the order of application in the defining language. As an example, we consider the definition of a simple applicative programming language by means of an interpreter written in a similar language. Definitions in each of the above classifications are derived from one another by informal but constructive methods. The treatment of imperative features such as jumps and assignment is also discussed."], "doi": "10.1.1.110.5892", "Keywords": ["higher-order programming language", "definitional interpreter", "lambda calculus", "simple applicative programming language", "defined language", "similar language", "value versus call", "applicative language", "secd machine", "reynolds definition", "constructive method", "imperative feature", "higher-order function", "mccarthy definition", "pure lisp", "defining language", "recent unpublished work", "vienna definition", "programming language"], "Labels": "programming languages"},
{"title": ["Software Engineering Economics  (1981) "], "Author": "BarryW.Boehm", "Abstract": ["Abstract\u2014This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation. Index Terms\u2014Computer programming costs, cost models, management decision"], "doi": "10.1.1.365.9642", "Keywords": ["software engineering economics", "software cost estimation", "major estimation technique", "software engineering", "management decision", "algorithmic cost model", "current state", "cost model", "recent trend", "economic analysis technique", "outstanding research issue", "index term computer programming cost"], "Labels": "software engineering"},
{"title": ["Cognitive networks  (2005) "], "Author": "RyanW.Thomas,LuizA.Dasilva,AllenB.Mackenzie", "Abstract": ["Abstract \u2014 This paper presents a definition and framework for a novel type of adaptive data network: the cognitive network. In a cognitive network, the collection of elements that make up the network observes network conditions and then, using prior knowledge gained from previous interactions with the network, plans, decides and acts on this information. Cognitive networks are different from other \u201cintelligent \u201d communication technologies because these actions are taken with respect to the end-to-end goals of a data flow. In addition to the cognitive aspects of the network, a specification language is needed to translate the user\u2019s end-to-end goals into a form understandable by the cognitive process. The cognitive network also depends on a Software Adaptable Network that has both an external interface accessible to the cognitive network and network status sensors. These devices are used to provide control and feedback. The paper concludes by presenting a simple case study to illustrate a cognitive network and its framework. I."], "doi": "10.1.1.722.8064", "Keywords": [], "Labels": "network communication"},
{"title": ["Reliable Communication in the Presence of Failures  (1987) "], "Author": "KennethP.Birman,ThomasA.Joseph", "Abstract": ["The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for fault-tolerant process groups in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach."], "doi": "10.1.1.106.6258", "Keywords": ["reliable communication", "fault-tolerant process group", "dynamic change", "valuable alternative", "conventional asynchronous communication protocol", "reliable multicast protocol", "causal delivery ordering", "application-specific delivery", "computer system", "level algorithm", "isi system", "wide-area network", "consistent ordering", "process failure", "several us", "high level", "fault-tolerant resilient object", "bulletin board", "group property", "communication facility", "member ranking", "significant simplification"], "Labels": "network communication"},
{"title": ["Cryptographic Hardware Architecture  (2002) "], "Author": "SavithriVenkatachalapathy", "Abstract": ["Abstract \u2014 In this paper we have tried to present a descriptive description of Cryptographic Hardware. Cryptography itself is a math that provides the security in many technologies, including all digital certificate and encryption systems. Either Software or Hardware implementations of cryptography can be used. The Hardware implementation is secure, faster, but comes at an exorbitant price. To this extent the current trends in Hardware implementations have a promising future for cost effective hardware crypto-systemss. In this regard we try to give a summary of Cryptography Processor Architecture, Dynamic implementation of one of the AES standards. For this, we have chosen the Serpernt Block Cipher algorithm. And finally the comparision of the performance of FPGA implementation of five standard AES algorithms and determine their suitability for FPGA implementation."], "doi": "10.1.1.110.5067", "Keywords": ["hardware implementation", "cryptographic hardware architecture", "fpga implementation", "cryptographic hardware", "exorbitant price", "aes standard", "promising future", "current trend", "dynamic implementation", "digital certificate", "serpernt block cipher algorithm", "encryption system", "cryptography processor architecture", "descriptive description", "many technology", "cost effective hardware crypto-systemss", "standard aes algorithm"], "Labels": "hardware architecture"},
{"title": ["Comparison of The hardware Architecture and FPGA "], "Author": "M.D.Galanis,P.Kitsos,G.Kostopoulos,N.Sklavos,O.Koufopavlou,C.E.Goutis", "Abstract": ["In this paper, the hardware implementations of five representative stream ciphers are compared in terms of performance and consumed area. The ciphers used for the comparison are the A5/1, W7, E0, RC4 and Helix. The first three ones have been used for the security part of wellknown standards. The Helix cipher is a recently introduced fast, word oriented, stream cipher. W7 algorithm has been proposed as a more trustworthy solution, due to the security problems that occurred concerning A5/1 strength. The designs were coded using VHDL language. For the hardware implementation of the designs, an FPGA device was used. The implementation results illustrate the hardware performance of each cipher in terms of throughput-to-area ratio. This ratio equals to: 5.88 for the A5/1, 1.26 for W7, 0.21 for the E0, 2.45 for the Helix and 0.86 for the RC4. 1."], "doi": "10.1.1.104.7846", "Keywords": ["hardware architecture", "hardware implementation", "consumed area", "implementation result", "fpga device", "helix cipher", "vhdl language", "hardware performance", "trustworthy solution", "wellknown standard", "throughput-to-area ratio", "representative stream cipher", "w7 algorithm", "stream cipher", "security problem", "security part"], "Labels": "hardware architecture"},
{"title": ["Information Filtering and Information Retrieval: Two Sides of the Same Coin  (1992) "], "Author": "NicholasJ.Belkin,W.BruceCroft", "Abstract": ["Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented."], "doi": "10.1.1.104.7500", "Keywords": ["information retrieval", "information filtering", "retrieval research", "represent long-term interest", "remote source", "multimedia information system", "textual information", "group information preference", "large amount", "data type", "structured data"], "Labels": "information retrieval"},
{"title": ["Abstract A Hardware Architecture for "], "Author": "CyrilFlaig,SurfaceSplatting,SimonHeinzle,SimonMall,TimoAila,KasparRohrer", "Abstract": ["We present a novel architecture for hardware-accelerated rendering of point primitives. Our pipeline implements a refined version of EWA splatting, a high quality method for antialiased rendering of point sampled representations. A central feature of our design is the seamless integration of the architecture into conventional, OpenGL-like graphics pipelines so as to complement triangle-based rendering. The specific properties of the EWA algorithm required a variety of novel design concepts including a ternary depth test and using an on-chip pipelined heap data structure for making the memory accesses of splat primitives more coherent. In addition, we developed a computationally stable evaluation scheme for perspectively corrected splats. We implemented our architecture both on reconfigurable FPGA boards and as an ASIC prototype, and we integrated it into an OpenGL-like software implementation. Our evaluation comprises a detailed performance analysis using scenes of varying complexity."], "doi": "10.1.1.84.3467", "Keywords": ["hardware architecture", "seamless integration", "point primitive", "opengl-like software implementation", "refined version", "triangle-based rendering", "opengl-like graphic pipeline", "asic prototype", "on-chip pipelined heap data structure", "central feature", "splat primitive", "reconfigurable fpga board", "memory access", "specific property", "antialiased rendering", "high quality method", "detailed performance analysis", "novel design concept", "novel architecture", "hardware-accelerated rendering", "ternary depth test", "ewa splatting", "stable evaluation scheme", "ewa algorithm"], "Labels": "hardware architecture"},
{"title": ["Value-Based Software Engineering  (2003) "], "Author": "BarryW.Boehm", "Abstract": ["Abstract\u2014This paper provides a definition of the term \u201csoftware engineering \u201d and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle\u2014requirements engineering, design, coding, test, and maintenance\u2014and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided. Index Terms\u2014Computer software, data systems, information systems,"], "doi": "10.1.1.365.9270", "Keywords": ["value-based software engineering", "software life cycle requirement engineering", "overall area", "integrated technology-management approach", "index term computer software", "current state", "various phase", "information system", "data system", "term software engineering", "likely future trend", "extensive set", "software management"], "Labels": "software engineering"},
{"title": ["No Silver Bullet: Essence and Accidents of Software Engineering  (1987) "], "Author": "FrederickP.Brooks", "Abstract": ["Of all the monsters that fill the nightmares of our folklore, none terrify more than werewolves, because they transform unexpectedly from the familiar into horrors. For these, one seeks bullets of silver that can magically lay them to rest. The familiar software project, at least as seen by the nontechnical manager, has something of this character; it is usually innocent and straightforward, but is capable of becoming a monster of missed schedules, blown budgets, and flawed products. So we hear desperate cries for a silver bullet--something to make software costs drop as rapidly as computer hardware costs do. But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed. Skepticism is not pessimism, however. Although we see no startling breakthroughs--and indeed, I believe such to be inconsistent with the nature of software--many encouraging"], "doi": "10.1.1.117.315", "Keywords": ["silver bullet", "software engineering", "seek bullet", "missed schedule", "computer hardware cost", "software many encouraging", "single development", "decade hence", "software problem", "management technique", "nontechnical manager", "order-of-magnitude improvement", "silver bullet something", "familiar software project", "desperate cry", "software cost drop"], "Labels": "software engineering"},
{"title": ["Requirements for Internet Hosts - Communication Layers  (1989) "], "Author": "R.Braden", "Abstract": ["This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. Distribution of this document is unlimited. Summary This is one RFC of a pair that defines and discusses the requirements for Internet host software. This RFC covers the communications protocol layers: link layer, IP layer, and transport layer; its companion RFC-1123 covers the application and support protocols."], "doi": "10.1.1.182.9099", "Keywords": ["internet host communication layer", "internet community", "companion rfc-1123", "primary protocol standard document", "ip layer", "support protocol", "internet host software", "official specification", "link layer"], "Labels": "network communication"},
{"title": [" Wireless sensor networks: a survey  (2002) "], "Author": "I.F.Akyildiz,W.Su,Y.Sankarasubramaniam,E.Cayirci", "Abstract": ["This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are"], "doi": "10.1.1.221.2247", "Keywords": ["sensor network", "wireless sensor network", "microelectro-mechanical system technology", "communication architecture", "potential sensor network application", "sensing task", "open research issue", "digital electronics", "wireless communication"], "Labels": "network communication"},
{"title": ["Enlisting Hardware Architecture  (2003) "], "Author": "ToThwartMalicious,Ru", "Abstract": ["Software vulnerabilities that enable the injection and execution of  malicious code in pervasive Internet-connected computing devices pose serious  threats to cyber security. In a common type of attack, a hostile party induces a  software buffer overflow in a susceptible computing device in order to corrupt a  procedure return address and transfer control to malicious code. These buffer  overflow attacks are often employed to recruit oblivious hosts into distributed  denial of service (DDoS) attack networks, which ultimately launch devastating  DDoS attacks against victim networks or machines. In spite of existing  software countermeasures that seek to prevent buffer overflow exploits, many  systems remain vulnerable."], "doi": "10.1.1.58.4399", "Keywords": ["hardware architecture", "malicious code", "attack network", "many system", "software vulnerability", "software countermeasure", "oblivious host", "distributed denial", "ddos attack", "software buffer overflow", "serious threat", "overflow attack", "susceptible computing device", "hostile party", "common type", "victim network", "procedure return address"], "Labels": "hardware architecture"},
{"title": ["An open graph visualization system and its applications to software engineering  (2000) "], "Author": "EmdenR.Gansner,StephenC.North", "Abstract": ["We describe a package of practical tools and libraries for manipulating graphs and their drawings. Our design, which aimed at facilitating the combination of the package components with other tools, includes stream and event interfaces for graph operations, high-quality static and dynamic layout algorithms, and the ability to handle sizable graphs. We conclude with a description of the applications of this package to a variety of software engineering tools."], "doi": "10.1.1.106.5621", "Keywords": ["software engineering", "open graph visualization system", "event interface", "graph operation", "software engineering tool", "dynamic layout algorithm", "sizable graph", "package component", "practical tool"], "Labels": "software engineering"},
{"title": ["Agent-Oriented Software Engineering  (1999) "], "Author": "MichaelWooldridge,P.Ciancarini", "Abstract": ["Software and knowledge... In this article, we argue that intelligent agents and agent-based systems offer novel opportunities for developing effective tools and techniques. Following a discussion on the classic subject of what makes software complex, we introduce intelligent agents as software structures capable of making \"rational decisions\". Such rational decision-makers are well-suited to the construction of certain types of software, which mainstream software engineering has had little success with. We then go on to examine a number of prototype techniques proposed for engineering agent systems, including formal specification and verification methods for agent systems, and techniques for implementing agent specifications"], "doi": "10.1.1.25.322", "Keywords": ["agent-oriented software engineering", "intelligent agent", "agent system", "effective tool", "certain type", "prototype technique", "classic subject", "rational decision-makers", "little success", "novel opportunity", "engineering agent system", "formal specification", "rational decision", "agent-based system", "agent specification", "software engineering", "verification method"], "Labels": "software engineering"},
{"title": ["Software Engineering Software Engineering  (1990) "], "Author": "IanSommerville", "Abstract": ["This paper describes how we have modified a software engineering stream within a computer science course to include broader concepts of systems engineering. We justify this inclusion by showing how many reported problems with large systems are not just software problems but relate to system issues such as hardware and operational processes. We describe what we mean by \u2018systems engineering \u2019 and go on to discuss the particular course structure which we have developed. We explain, in some detail, the contents of two specific systems engineering courses (Software Intensive Systems Engineering and Critical Systems Engineering) and discuss the problems and challenges we have faced in making these changes. In the Appendix, we provide details of the case studies which are used as linking themes in our courses. 1."], "doi": "10.1.1.128.1760", "Keywords": ["software engineering software engineering", "system engineering", "large system", "software engineering stream", "computer science course", "software intensive system engineering", "software problem", "operational process", "case study", "system issue", "specific system engineering course", "critical system engineering", "particular course structure"], "Labels": "software engineering"},
{"title": ["Wireless Ad Hoc Networks  (2002) "], "Author": "ZygmuntJ.Haas,JingDeng,BenLiang,PanagiotisPapadimitratos,S.Sajama", "Abstract": ["A mobile ad hoc network is a relatively new term for an old technology - a network that does not rely on pre-existing infrastructure. Roots of this technology could be traced back to the early 1970s with the DARPA PRNet and the SURAN projects. The new twitch is the application of this technology in the non-military communication environments. Additionally, the research community has also recently addressed some extended features of this technology, such as multicasting and security. Also numerous new solutions to the \"old\" problems of routing and medium access control have been proposed. This survey attempts to summarize the state-ofthe -art of the ad hoc networking technology in four areas: routing, medium access control, multicasting, and security. Where possible, comparison between the proposed protocols is also discussed."], "doi": "10.1.1.13.8133", "Keywords": [], "Labels": "network communication"},
{"title": ["Search-Based Software Engineering "], "Author": "MarkHarman,BryanF.Jones", "Abstract": ["... The paper briefly sets out key ingredients for successful reformulation and evaluation criteria for Search-Based Software Engineering. "], "doi": "10.1.1.143.9716", "Keywords": ["search-based software engineering", "key ingredient", "paper briefly", "successful reformulation", "evaluation criterion"], "Labels": "software engineering"},
{"title": ["Fundamentals of Software Engineering  (1991) "], "Author": "MehdiJazayeri,TuWien,TuWien", "Abstract": ["What is software engineering n Software? \u2013 Makes computer do specific task n Engineering? \u2013 Building machines to do useful things n Science \u2013 Discovering rules of nature n Technology"], "doi": "10.1.1.197.7962", "Keywords": ["software engineering", "useful thing science discovering rule", "building machine", "specific task", "software engineering software", "nature technology", "make computer"], "Labels": "software engineering"},
{"title": ["Experimentation in software engineering  (1986) "], "Author": "VictorR.Basili,RichardW.Selly,DavidH.HutchensAo,VictorR.Basili,RichardW.Sel", "Abstract": ["9. 3P#LQ w1X1OA1G AGIXC? iMA) AMQ ADOQAU15I) 11. IPQNSOIBjrM~mWm\""], "doi": "10.1.1.475.8699", "Keywords": ["software engineering", "ipqnsoibjrm mwm", "lq w1x1oa1g agixc", "amq adoqau15i"], "Labels": "software engineering"},
{"title": ["How practical is network coding?  (2006) "], "Author": "MeaWang,BaochunLi", "Abstract": ["  With network coding, intermediate nodes between the source and the receivers of an end-to-end communication session are not only capable of relaying and replicating data messages, but also of coding incoming messages to produce coded outgoing ones. Recent studies have shown that network coding is beneficial for peer-to-peer content distribution, since it eliminates the need for content reconciliation, and is highly resilient to peer failures. In this paper, we present our recent experiences with a highly optimized and high-performance C++ implementation of randomized network coding at the application layer. We present our observations based on an extensive series of experiments, draw conclusions from a wide range of scenarios, and are more cautious and less optimistic as compared to previous studies.  "], "doi": "10.1.1.334.8287", "Keywords": ["linear network", "network coding", "recent study", "previous study", "data message", "high-performance implementation", "randomized network", "intermediate node", "peer-to-peer content distribution", "application layer", "end-to-end communication session", "wide range", "extensive series", "draw conclusion", "content reconciliation", "recent experience"], "Labels": "network communication"}
]